---
title: "Apache Beam SQL"
date: 2022-09-14
categories: [Stream Processing, Apache Beam, SQL, GCP, Dataflow]
tagline: ""
header:
    overlay_image: /assets/images/marek-piwnicki-6GF7BeT8g30-unsplash.webp
    overlay_filter: 0.2
---

If you are a BigData engineer who develops batch data pipelines, 
you might often hear that stream processing is a future.
It unlocks the full potential of data that is often unbounded in nature. 
You don't need batch pipelines anymore, everything could be implemented in a streaming fashion. 
There are plenty of modern and easy to use tools for stream processing. 
Let's try :)

In this blog post you will learn:

* How to implement streaming data pipeline with [Apache Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) 
  and deploy it on the Google Cloud Platform using [Dataflow](https://cloud.google.com/dataflow) runner.
* The importance of the [watermark](https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#watermarks) to calculate the correct results within the minimal latency.
* What is the impact of late data for the streaming pipelines.
* A few undocumented Dataflow features :)

## Streaming Pipeline Overview

The streaming data pipeline for IoT like system is presented below: 

{% mermaid %}
flowchart LR
Sensor1 --> A1[Unreliable Network] --> A2[/Pubsub topic 1/] --> A3[Tumble Window]
Sensor2 --> B1[Unreliable Network] --> B2[/Pubsub topic 2/] --> B3[Tumble Window]
subgraph Dataflow SQL
A3[Tumble Window] --> C1(Left Outer Join)
B3[Tumble Window] --> C1(Left Outer Join)
end
C1(Left Outer Join) --> C2[(BigQuery Table)]
{% endmermaid %}

There are two sensors that send their readings periodically over unreliable network to Pubsub topics. 
The payload looks like:

```json
{'ts': '2022-08-07T10:28:28+00:00', 'val': 0.69}
{'ts': '2022-08-07T10:28:29+00:00', 'val': 0.67}
{'ts': '2022-08-07T10:28:30+00:00', 'val': 0.89}
```

Next, there is a Dataflow SQL job which subscribes to the topics and calculate the averages of the sensor readings in short non overlapping fixed windows (tumble windows).
The averages from both sensors are joined and the job calculates the difference between them.
The readings from the right stream are optional, so if there is no data from second sensor the calculation is done within data from the first sensor only.
Finally, the results of the computation are written continuously to BigQuery table for further analysis.

The query:

```sql
SELECT 
    CURRENT_TIMESTAMP, s1.window_start, s1.window_end, 
    s1.samples + COALESCE(s2.samples, 0),
    s1.value_avg - COALESCE(s2.value_avg, 0.0)       
FROM (
    SELECT
        TUMBLE_START(event_timestamp, INTERVAL '5' SECOND) AS window_start,
        TUMBLE_END(event_timestamp, INTERVAL '5' SECOND) AS window_end,
        AVG(stream1.payload.val) AS value_avg,
        COUNT(*) AS samples
    FROM stream1
    GROUP BY TUMBLE(event_timestamp, INTERVAL '5' SECOND)) s1
LEFT OUTER JOIN (
    SELECT
        TUMBLE_START(event_timestamp, INTERVAL '5' SECOND) AS window_start,
        TUMBLE_END(event_timestamp, INTERVAL '5' SECOND) AS window_end,
        AVG(stream2.payload.val) AS value_avg,
        COUNT(*) AS samples
    FROM stream2
    GROUP BY TUMBLE(event_timestamp, INTERVAL '5' SECOND)) s2
ON s1.window_start = s2.window_start AND s1.window_end = s2.window_end 
```

## Pubsub topics

Create Pubsub topics where stream of sensor data will be published:

```shell
$ gcloud pubsub topics create marcin-atm22-stream-1
Created topic [projects/sc-9366-nga-dev/topics/marcin-atm22-stream-1].
$ gcloud pubsub topics create marcin-atm22-stream-2
Created topic [projects/sc-9366-nga-dev/topics/marcin-atm22-stream-2].
```

## SQL Shell

Starting with version 2.6.0 Beam SQL includes an interactive shell, called the Beam SQL shell.
The shell allows you to write the pipelines as SQL queries and execute the queries on Direct runner.
You can also specify different runner, e.g. Dataflow and execute the pipeline on Google Cloud Platform.

The latest working version of Apache Beam SQL shell is 2.38.
See [https://github.com/apache/beam/issues/22615](https://github.com/apache/beam/issues/22615) for more details.
{: .notice--warning}

To use Beam SQL shell you must build the binary from sources. 
Clone Apache Beam [repository](https://github.com/apache/beam) and execute the following command 
to build the SQL shell with support for Dataflow runner and Google IO connectors.

```shell
$ ./gradlew -p sdks/java/extensions/sql/shell \
    -Pbeam.sql.shell.bundled=':sdks:java:io:google-cloud-platform,:runners:google-cloud-dataflow-java' \
    installDist
(...)
BUILD SUCCESSFUL in 22s
88 actionable tasks: 43 executed, 35 from cache, 10 up-to-date
```

Run Apache Beam SQL shell and check for registered tables:

```shell
$ ./sdks/java/extensions/sql/shell/build/install/shell/bin/shell
Welcome to Beam SQL 2.38.0-SNAPSHOT (based on sqlline version 1.4.0)
0: BeamSQL> !tables
+-----------+-------------+------------+--------------+---------+----------+------------+
| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | TABLE_TYPE   | REMARKS | TYPE_CAT | TYPE_SCHEM |
+-----------+-------------+------------+--------------+---------+----------+------------+
|           | metadata    | COLUMNS    | SYSTEM TABLE |         |          |            |
|           | metadata    | TABLES     | SYSTEM TABLE |         |          |            |
+-----------+-------------+------------+-----------  -+---------+----------+------------+
0: BeamSQL> !quit
Closing: org.apache.beam.sdk.extensions.sql.impl.JdbcConnection
```

## External tables

With Beam SQL extension you can map Pubsub topics and BigQuery tables as external tables. 
It is required to use external storage systems in SQL queries. 

Create external tables for Pubsub topics:

```shell
0: BeamSQL> CREATE EXTERNAL TABLE stream1 (
. . . . . >   event_timestamp TIMESTAMP, attributes MAP<VARCHAR, VARCHAR>, 
. . . . . >   payload ROW<ts TIMESTAMP, val DOUBLE>)
. . . . . > TYPE pubsub
. . . . . > LOCATION 'projects/sc-9366-nga-dev/topics/marcin-atm22-stream-1'
. . . . . > TBLPROPERTIES '{"format": "json", "timestampAttributeKey":"ts"}';
No rows affected (0.819 seconds)
0: BeamSQL> CREATE EXTERNAL TABLE stream2 (
. . . . . >   event_timestamp TIMESTAMP, attributes MAP<VARCHAR, VARCHAR>, 
. . . . . >   payload ROW<ts TIMESTAMP, val DOUBLE>)
. . . . . > TYPE pubsub
. . . . . > LOCATION 'projects/sc-9366-nga-dev/topics/marcin-atm22-stream-2'
. . . . . > TBLPROPERTIES '{"format": "json", "timestampAttributeKey":"ts"}';
No rows affected (0.019 seconds)
```

Pay special attention for `timestampAttributeKey` property, the Pubsub attribute used for tracking event time.
{: .notice--info}

Create external BigQuery table for the results:

```shell
0: BeamSQL> CREATE EXTERNAL TABLE results (
. . . . . >   created_at TIMESTAMP, window_start TIMESTAMP, window_end TIMESTAMP, 
. . . . . >   samples INT, avg_diff DOUBLE)
. . . . . > TYPE bigquery
. . . . . > LOCATION 'sc-9366-nga-dev:marcin_atm22.results'
. . . . . > TBLPROPERTIES '{"writeDisposition": "WRITE_TRUNCATE"}';
No rows affected (0.013 seconds)
```

Check the table schema:

```shell
0: BeamSQL> !describe results
+-----------+-------------+------------+--------------+-----------+--------------+
| TABLE_CAT | TABLE_SCHEM | TABLE_NAME | COLUMN_NAME  | DATA_TYPE | TYPE_NAME    |
+-----------+-------------+------------+--------------+-----------+--------------+
|           | beam        | results    | created_at   | 93        | TIMESTAMP(6) |
|           | beam        | results    | window_start | 93        | TIMESTAMP(6) |
|           | beam        | results    | window_end   | 93        | TIMESTAMP(6) |
|           | beam        | results    | samples      | 4         | INTEGER      |
|           | beam        | results    | avg_diff     | 8         | DOUBLE       |
+-----------+-------------+------------+--------------+-----------+--------------+
```

Try to query the table. 
You will get an error because table does not exist yet, it will be created just before first insert.

```shell
0: BeamSQL> SELECT COUNT(*) from RESULTS;
Error: Error while executing SQL "SELECT COUNT(*) from RESULTS": com.google.api.gax.rpc.NotFoundException: io.grpc.StatusRuntimeException: NOT_FOUND: 
Not found: Table sc-9366-nga-dev:marcin_atm22.results (state=,code=0)
```

## SQL pipeline

Change default Direct runner to Dataflow runner and set a few mandatory options:

```shell
0: BeamSQL> SET runner = DataflowRunner;
No rows affected (0.017 seconds)
0: BeamSQL> set region = 'europe-west1';
No rows affected (0.004 seconds)
0: BeamSQL> set gcpTempLocation = 'gs://sc-9366-nga-dev-marcin/tmp';
No rows affected (0.015 seconds)
```

Execute final query and wait a few minutes until Dataflow job creates subscriptions and connect to Pubsub topics:

```shell
0: BeamSQL> INSERT INTO results
TODO: put final query
. . . . . > ;
(...)
Executing pipeline on the Dataflow Service, which will have billing implications related to Google Compute Engine usage and other Google Cloud Services.
To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/europe-west1/2022-08-08_11_52_01-1894085106596231231?project=sc-9366-nga-dev
(...)
INFO: To cancel the job using the 'gcloud' tool, run:
> gcloud dataflow jobs --project=sc-9366-nga-dev cancel --region=europe-west1 2022-08-07_04_04_14-1079245398928473428
No rows affected (26.455 seconds)
```

The job might be also deployed with `gcloud dataflow sql query` command, see the [official guide](https://cloud.google.com/dataflow/docs/guides/sql/dataflow-sql-intro).
{: .notice--info}

## Data generator

For testing purposes a synthetic data generator is used instead of readings from the real sensors:

* The generator publishes the event every second.
* You can specify the destination Pubsub topic.
* You can also define a delay of generated events for testing late data scenarios.
* As a homework you can introduce some variability, generate non-monotonic readings and make a computation even more tricky.

```python
import argparse
import json
from datetime import datetime, timezone, timedelta
from random import random
from time import sleep

from google.cloud import pubsub_v1


def generate_event(delay):
    ts = datetime.now(tz=timezone.utc) - timedelta(seconds=delay)
    return {
        'ts': ts.isoformat(timespec='seconds'),
        'val': round(random(), 2)
    }


def publish_event(publisher, topic, event):
    data = json.dumps(event).encode('utf-8')
    publisher.publish(topic, data=data, ts=event['ts'])


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--topic', required=True)
    parser.add_argument("--delay", default=0, type=int)
    args = parser.parse_args()

    publisher = pubsub_v1.PublisherClient()

    while True:
        event = generate_event(args.delay)
        print(event)
        publish_event(publisher, args.topic, event)
        sleep(1)
```

Do you remember `timestampAttributeKey` property for Pubsub external table definition? 
Generator sets `ts` attribute for each published message based on event time.
{: .notice--info}


Start data generators for topics `stream-1` and `stream-2`:

```
$ python generator.py --topic projects/sc-9366-nga-dev/topics/marcin-atm22-stream-1
{'ts': '2022-08-07T10:28:28+00:00', 'val': 0.69}
{'ts': '2022-08-07T10:28:29+00:00', 'val': 0.67}
{'ts': '2022-08-07T10:28:30+00:00', 'val': 0.89}
(...)
```

```
$ python generator.py --topic projects/sc-9366-nga-dev/topics/marcin-atm22-stream-2
{'ts': '2022-08-07T10:29:28+00:00', 'val': 0.53}
{'ts': '2022-08-07T10:29:29+00:00', 'val': 0.76}
{'ts': '2022-08-07T10:29:30+00:00', 'val': 0.94}
(...)
```

Configure Beam SQL to use Direct runner again:

```shell
0: BeamSQL> SET runner = DirectRunner;
No rows affected (0.017 seconds)
```

Try to execute SQL query on Pubsub topics:

```shell
0: BeamSQL> SELECT current_timestamp, event_timestamp, stream1.payload.ts, stream1.payload.val FROM stream1 LIMIT 5;
+---------------------+---------------------+---------------------+------+
| current_timestamp   | event_timestamp     |   ts                | val  |
+---------------------+---------------------+---------------------+------+
| 2022-08-07 10:39:13 | 2022-08-07 10:39:07 | 2022-08-07 10:39:07 | 0.14 |
| 2022-08-07 10:39:13 | 2022-08-07 10:39:05 | 2022-08-07 10:39:05 | 0.2  |
| 2022-08-07 10:39:13 | 2022-08-07 10:39:09 | 2022-08-07 10:39:09 | 0.05 |
| 2022-08-07 10:39:13 | 2022-08-07 10:39:08 | 2022-08-07 10:39:08 | 0.95 |
| 2022-08-07 10:39:13 | 2022-08-07 10:39:01 | 2022-08-07 10:39:01 | 0.63 |
+---------------------+---------------------+---------------------+------+
5 rows selected (26.292 seconds)
0: BeamSQL> SELECT current_timestamp, event_timestamp, stream2.payload.ts, stream2.payload.val FROM stream2 LIMIT 5;
+---------------------+---------------------+---------------------+------+
| current_timestamp   | event_timestamp     |   ts                | val  |
+---------------------+---------------------+---------------------+------+
| 2022-08-07 10:42:52 | 2022-08-07 10:42:49 | 2022-08-07 10:42:49 | 0.99 |
| 2022-08-07 10:42:52 | 2022-08-07 10:42:50 | 2022-08-07 10:42:50 | 0.36 |
| 2022-08-07 10:42:52 | 2022-08-07 10:42:47 | 2022-08-07 10:42:47 | 0.78 |
| 2022-08-07 10:42:52 | 2022-08-07 10:42:48 | 2022-08-07 10:42:48 | 0.52 |
| 2022-08-07 10:42:52 | 2022-08-07 10:42:40 | 2022-08-07 10:42:40 | 0.72 |
+---------------------+---------------------+---------------------+------+
5 rows selected (16.239 seconds)
```

## Dataflow job

Now, it's time to examine logical graph of the Dataflow job. 
It should look like the graph presented below:

![Dataflow logical graph](/assets/images/beam_sql_logical_graph.png)

* IO source steps (424, 429) for Pubsub subscriptions
* Calculation steps (579, 586) for decoding payload
* Aggregation steps (581, 588) for windowing
* Calculation steps (583, 590) for calculating averages
* Co GBK (Group By Key) step (592) for joining streams
* Final results calculation step (594)
* IO sink step (594) for saving results to BigQuery table

Dataflow optimizes the logical graph into physical execution graph.
As you can see some steps are fused together but the overall shape of the graph stays unchanged.

![Dataflow execution graph](/assets/images/beam_sql_execution_graph_1.png)

Examine execution graph for data freshness:

* There is a 19-20 seconds of delay for initial steps responsible for reading and processing data from Pubsub topics.
* Windowing, and saving to BigQuery introduces another 11 or 12 seconds of latency.

Finally, examine the results generated by the job by querying BigQuery table:

```shell
0: BeamSQL> SELECT
. . . . . >   window_start,
. . . . . >   window_end,
. . . . . >   created_at,
. . . . . >   TIMESTAMPDIFF(SECOND, created_at, window_end) AS delay_secs,
. . . . . >   samples AS no_of_samples,
. . . . . >   ROUND(avg_diff, 2) AS avg_diff
. . . . . > FROM results
. . . . . > ORDER BY window_end DESC
. . . . . > LIMIT 5;
+---------------------+---------------------+---------------------+------------+---------------+----------+
| window_start        | window_end          | created_at          | delay_secs | no_of_samples | avg_diff |
+---------------------+---------------------+---------------------+------------+---------------+----------+
| 2022-08-08 19:07:10 | 2022-08-08 19:07:15 | 2022-08-08 19:07:43 | -28        | 10            | 0.34     |
| 2022-08-08 19:07:05 | 2022-08-08 19:07:10 | 2022-08-08 19:07:36 | -26        | 10            | 0.12     |
| 2022-08-08 19:07:00 | 2022-08-08 19:07:05 | 2022-08-08 19:07:32 | -27        | 10            | -0.06    |
| 2022-08-08 19:06:55 | 2022-08-08 19:07:00 | 2022-08-08 19:07:30 | -30        | 10            | -0.23    |
| 2022-08-08 19:06:50 | 2022-08-08 19:06:55 | 2022-08-08 19:07:22 | -27        | 10            | 0.25     |
+---------------------+---------------------+---------------------+------------+---------------+----------+
```

The results for sensor readings from the window `[19:07:10-19:07:15)` are saved at `19:07:43` what gives ~28 seconds of end-to-end latency.

TODO: explain that the latency is rather 28-5=23 seconds or not?

## Late data (but not too late)

Let's simulate late data from a phantom sensor.
Start second generator for `stream-2` topic but with configured delay of 30 seconds:

```shell
$ python generator.py --topic projects/sc-9366-nga-dev/topics/marcin-atm22-stream-2 --delay 30
```

Examine execution graph for data freshness:

![Dataflow execution graph](/assets/images/beam_sql_execution_graph_2.png)

TODO: show watermarks!

* Dataflow recognized late data and hold watermark for one of the input stream (data freshness 40-50 seconds).
* Moreover, the latency was propagated downstream (end-to-end data freshness is ~50 seconds). The window might be finalized only if data from both streams has been processed. 

TODO: better explanation

Examine results:

```shell
TODO: results with higher delay_secs
```

For larger throughput you could also observe, that Dataflow runner slows down the consumption from up-to-date subscription.
The input with late data drives the throughput of the whole job.

Streaming runner must provide some kind of back-pressure mechanism to align the processing to the data freshness.
{: .notice--info}

TODO: better explanation of back-pressure

## Tracking subscription

TODO: better explanation

How Dataflow is able to estimate data freshness if it stops consuming data due to lateness in another sources?
Dataflow creates additional subscription just for event time tracking.
For our SQL pipeline there are four subscriptions instead of two!

![Dataflow tracking subscriptions](/assets/images/beam_sql_tracking_subscription.png)

Tracking subscriptions double Pubsub Message Delivery costs.
Why have we to pay for the whole message delivery if only timestamp attribute is enough for tracking? I don't know.
{: .notice--info}

## Missing data

Stop all generators for `stream-2` topic, and watch the execution graph again:

1. The watermark of the job is hold on the time of the latest published event in the `stream-2`
2. The data freshness is decreasing
3. But after ~3 minutes the magic happens, the watermark is released again and follow the time of the events in the `stream-1`. 
   Data freshness for both streams are the same (~10-20 seconds).

You should observe similar results:

```
+---------------------+---------------------+---------------------+------------+---------------+----------+
| window_start        | window_end          | created_at          | delay_secs | no_of_samples | avg_diff |
+---------------------+---------------------+---------------------+------------+---------------+----------+
| 2022-08-08 19:07:10 | 2022-08-08 19:07:15 | 2022-08-08 19:07:43 | -28        | 10            | 0.34     |
| 2022-08-08 19:07:05 | 2022-08-08 19:07:10 | 2022-08-08 19:07:36 | -26        | 10            | 0.12     |
| 2022-08-08 19:07:00 | 2022-08-08 19:07:05 | 2022-08-08 19:07:32 | -27        | 10            | -0.06    |
| 2022-08-08 19:06:55 | 2022-08-08 19:07:00 | 2022-08-08 19:07:30 | -30        | 10            | -0.23    |
| 2022-08-08 19:06:50 | 2022-08-08 19:06:55 | 2022-08-08 19:07:22 | -27        | 10            | 0.25     |
+---------------------+---------------------+---------------------+------------+---------------+----------+
```

If there is no traffic on the Pubsub topic for ~3 minutes, Dataflow assumes that the stream is fully up-to-date and release the watermark.
{: .notice--info}

## Clean up

Apache Beam SQL execute job on single `n1-standard-4` worker.
Remember to stop the job to avoid unnecessary costs.

```shell
$ gcloud dataflow jobs --project=sc-9366-nga-dev cancel --region=europe-west1 2022-08-07_04_04_14-1079245398928473428
Cancelled job [2022-08-07_04_04_14-1079245398928473428]
```

## Summary

Pros:
* Developing the complete streaming pipeline as SQL query does not require developer competences.
* SQL pipeline deployment is also quite easy, you could use Beam SQL Shell of `gcloud` command.
* Dataflow UI console is amazing, you will get everything you need (metrics, logs) in the single place.

Cons:
* Streaming pipelines troubleshooting is complex, you have to understand watermarks, latency propagation and more.
* You can not express more complex streaming building blocks like triggering, stateful DoFn as SQL query.
* If you need streaming pipeline deployed on production you will have to find skilled BigData engineering team.
