<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Spark | Passionate Developer]]></title>
  <link href="http://mkuthan.github.io/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://mkuthan.github.io/"/>
  <updated>2017-11-26T21:35:57+00:00</updated>
  <id>http://mkuthan.github.io/</id>
  <author>
    <name><![CDATA[Marcin Kuthan]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Long-running Spark Streaming Jobs on YARN Cluster]]></title>
    <link href="http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/"/>
    <updated>2016-09-30T00:00:00+00:00</updated>
    <id>http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn</id>
    <content type="html"><![CDATA[<p>A long-running Spark Streaming job, once submitted to the YARN cluster should run forever until it is intentionally stopped.
Any interruption introduces substantial processing delays and could lead to data loss or duplicates.
Neither YARN nor Apache Spark have been designed for executing long-running services.
But they have been successfully adapted to growing needs of near real-time processing implemented as long-running jobs.
Successfully does not necessarily mean without technological challenges.</p>

<p>This blog post summarizes my experiences in running mission critical, long-running Spark Streaming jobs on a secured YARN cluster.
You will learn how to submit Spark Streaming application to a YARN cluster to avoid sleepless nights during on-call hours.</p>

<h2>Fault tolerance</h2>

<p>In the YARN cluster mode Spark driver runs in the same container as the Application Master,
the first YARN container allocated by the application.
This process is responsible for driving the application and requesting resources (Spark executors) from YARN.
What is important, Application Master eliminates need for any another process that run during application lifecycle.
Even if an edge Hadoop cluster node where the Spark Streaming job was submitted fails, the application stays unaffected.</p>

<p>To run Spark Streaming application in the cluster mode, ensure that the following parameters are given to spark-submit command:</p>

<p><code>
spark-submit --master yarn --deploy-mode cluster
</code></p>

<p>Because Spark driver and Application Master share a single JVM, any error in Spark driver stops our long-running job.
Fortunately it is possible to configure maximum number of attempts that will be made to re-run the application.
It is reasonable to set higher value than default 2 (derived from YARN cluster property <code>yarn.resourcemanager.am.max-attempts</code>).
For me 4 works quite well, higher value may cause unnecessary restarts even if the reason of the failure is permanent.</p>

<p>```
spark-submit &mdash;master yarn &mdash;deploy-mode cluster \</p>

<pre><code>--conf spark.yarn.maxAppAttempts=4
</code></pre>

<p>```</p>

<p>If the application runs for days or weeks without restart or redeployment on highly utilized cluster, 4 attempts could be exhausted in few hours.
To avoid this situation, the attempt counter should be reset on every hour of so.</p>

<p>```
spark-submit &mdash;master yarn &mdash;deploy-mode cluster \</p>

<pre><code>--conf spark.yarn.maxAppAttempts=4 \
--conf spark.yarn.am.attemptFailuresValidityInterval=1h
</code></pre>

<p>```</p>

<p>Another important setting is a maximum number of executor failures before the application fails.
By default it is <code>max(2 * num executors, 3)</code>, well suited for batch jobs but not for long-running jobs.
The property comes with corresponding validity interval which also should be set.</p>

<p>```
spark-submit &mdash;master yarn &mdash;deploy-mode cluster \</p>

<pre><code>--conf spark.yarn.maxAppAttempts=4 \
--conf spark.yarn.am.attemptFailuresValidityInterval=1h \
--conf spark.yarn.max.executor.failures={8 * num_executors} \
--conf spark.yarn.executor.failuresValidityInterval=1h
</code></pre>

<p>```</p>

<p>For long-running jobs you could also consider to boost maximum number of task failures before giving up the job.
By default tasks will be retried 4 times and then job fails.</p>

<p>```
spark-submit &mdash;master yarn &mdash;deploy-mode cluster \</p>

<pre><code>--conf spark.yarn.maxAppAttempts=4 \
--conf spark.yarn.am.attemptFailuresValidityInterval=1h \
--conf spark.yarn.max.executor.failures={8 * num_executors} \
--conf spark.yarn.executor.failuresValidityInterval=1h \
--conf spark.task.maxFailures=8
</code></pre>

<p>```</p>

<h2>Performance</h2>

<p>When a Spark Streaming application is submitted to the cluster, YARN queue where the job runs must be defined.
I strongly recommend using YARN <a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
and submitting long-running jobs to separate queue.
Without a separate YARN queue your long-running job will be preempted by a massive Hive query sooner or later.</p>

<p>```
spark-submit &mdash;master yarn &mdash;deploy-mode cluster \</p>

<pre><code>--conf spark.yarn.maxAppAttempts=4 \
--conf spark.yarn.am.attemptFailuresValidityInterval=1h \
--conf spark.yarn.max.executor.failures={8 * num_executors} \
--conf spark.yarn.executor.failuresValidityInterval=1h \
--conf spark.task.maxFailures=8 \
--queue realtime_queue
</code></pre>

<p>```
Another important issue for Spark Streaming job is keeping processing time stable and highly predictable.
Processing time should stay below batch duration to avoid delays.
I&rsquo;ve found that Spark speculative execution helps a lot, especially on a busy cluster.
Batch processing times are much more stable when speculative execution is enabled.
Unfortunately speculative mode can be enabled only if Spark actions are idempotent.</p>

<p>```
spark-submit &mdash;master yarn &mdash;deploy-mode cluster \</p>

<pre><code>--conf spark.yarn.maxAppAttempts=4 \
--conf spark.yarn.am.attemptFailuresValidityInterval=1h \
--conf spark.yarn.max.executor.failures={8 * num_executors} \
--conf spark.yarn.executor.failuresValidityInterval=1h \
--conf spark.task.maxFailures=8 \
--queue realtime_queue \
--conf spark.speculation=true
</code></pre>

<p>```</p>

<h2>Security</h2>

<p>On a secured HDFS cluster, long-running Spark Streaming jobs fails due to Kerberos ticket expiration.
Without additional settings, Kerberos ticket is issued when Spark Streaming job is submitted to the cluster.
When ticket expires Spark Streaming job is not able to write or read data from HDFS anymore.</p>

<p>In theory (based on documentation) it should be enough to pass Kerberos principal and keytab as spark-submit command:</p>

<p>```
spark-submit &mdash;master yarn &mdash;deploy-mode cluster \</p>

<pre><code> --conf spark.yarn.maxAppAttempts=4 \
 --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
 --conf spark.yarn.max.executor.failures={8 * num_executors} \
 --conf spark.yarn.executor.failuresValidityInterval=1h \
 --conf spark.task.maxFailures=8 \
 --queue realtime_queue \
 --conf spark.speculation=true \
 --principal user/hostname@domain \
 --keytab /path/to/foo.keytab
</code></pre>

<p>```</p>

<p>In practice, due to several bugs (<a href="https://issues.apache.org/jira/browse/HDFS-9276">HDFS-9276</a>, <a href="https://issues.apache.org/jira/browse/SPARK-11182">SPARK-11182</a>)
HDFS cache must be disabled. If not, Spark will not be able to read updated token from file on HDFS.</p>

<p>```
spark-submit &mdash;master yarn &mdash;deploy-mode cluster \</p>

<pre><code> --conf spark.yarn.maxAppAttempts=4 \
 --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
 --conf spark.yarn.max.executor.failures={8 * num_executors} \
 --conf spark.yarn.executor.failuresValidityInterval=1h \
 --conf spark.task.maxFailures=8 \
 --queue realtime_queue \
 --conf spark.speculation=true \
 --principal user/hostname@domain \
 --keytab /path/to/foo.keytab \
 --conf spark.hadoop.fs.hdfs.impl.disable.cache=true
</code></pre>

<p>```</p>

<p>Mark Grover pointed out that those bugs only affect HDFS cluster configured with NameNodes in
<a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">HA mode</a>.
Thanks, Mark.</p>

<h2>Logging</h2>

<p>The easiest way to access Spark application logs is to configure Log4j console appender,
wait for application termination and use <code>yarn logs -applicationId [applicationId]</code> command.
Unfortunately it is not feasible to terminate long-running Spark Streaming jobs to access the logs.</p>

<p>I recommend to install and configure Elastic, Logstash and Kibana (<a href="https://www.elastic.co/">ELK</a> stack).
ELK installation and configuration is out of this blog post scope,
but remember to log the following context fields:</p>

<ul>
<li>YARN application id</li>
<li>YARN container hostname</li>
<li>Executor id (Spark driver is always 000001, Spark executors start from 000002)</li>
<li>YARN attempt (to check how many times Spark driver has been restarted)</li>
</ul>


<p>Log4j configuration with Logstash specific appender and layout definition should be passed to spark-submit command:</p>

<p>```
spark-submit &mdash;master yarn &mdash;deploy-mode cluster \</p>

<pre><code> --conf spark.yarn.maxAppAttempts=4 \
 --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
 --conf spark.yarn.max.executor.failures={8 * num_executors} \
 --conf spark.yarn.executor.failuresValidityInterval=1h \
 --conf spark.task.maxFailures=8 \
 --queue realtime_queue \
 --conf spark.speculation=true \
 --principal user/hostname@domain \
 --keytab /path/to/foo.keytab \
 --conf spark.hadoop.fs.hdfs.impl.disable.cache=true \
 --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties \
 --conf spark.executor.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties \
 --files /path/to/log4j.properties
</code></pre>

<p>```</p>

<p>Finally Kibana dashboard for Spark Job might look like:</p>

<p><img src="/images/blog/spark_job_logging.png" title="[spark job logging]" ></p>

<h2>Monitoring</h2>

<p>Long running job runs 24/7 so it is important to have an insight into historical metrics.
Spark UI keeps statistics only for limited number of batches, and after restart all metrics are gone.
Again, external tools are needed.
I recommend to install <a href="https://graphiteapp.org/">Graphite</a> for collecting metrics
and <a href="http://grafana.org/">Grafana</a> for building dashboards.</p>

<p>First, Spark needs to be configured to report metrics into Graphite, prepare the <code>metrics.properties</code> file:</p>

<p>```
<em>.sink.graphite.class=org.apache.spark.metrics.sink.GraphiteSink
</em>.sink.graphite.host=[hostname]
<em>.sink.graphite.port=[port]
</em>.sink.graphite.prefix=some_meaningful_name</p>

<p>driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource
executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource
```</p>

<p>And configure spark-submit command:</p>

<p>```
spark-submit &mdash;master yarn &mdash;deploy-mode cluster \</p>

<pre><code> --conf spark.yarn.maxAppAttempts=4 \
 --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
 --conf spark.yarn.max.executor.failures={8 * num_executors} \
 --conf spark.yarn.executor.failuresValidityInterval=1h \
 --conf spark.task.maxFailures=8 \
 --queue realtime_queue \
 --conf spark.speculation=true \
 --principal user/hostname@domain \
 --keytab /path/to/foo.keytab \
 --conf spark.hadoop.fs.hdfs.impl.disable.cache=true \
 --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties \
 --conf spark.executor.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties \
 --files /path/to/log4j.properties:/path/to/metrics.properties
</code></pre>

<p>```</p>

<h3>Metrics</h3>

<p>Spark publishes tons of metrics from driver and executors.
If I were to choose the most important one, it would be the last received batch records.
When <code>StreamingMetrics.streaming.lastReceivedBatch_records == 0</code> it probably means that Spark Streaming job has been stopped or failed.</p>

<p>Other important metrics are listed below:</p>

<ul>
<li>When total delay is greater than batch interval, latency of the processing pipeline increases.</li>
</ul>


<p><code>
driver.StreamingMetrics.streaming.lastCompletedBatch_totalDelay
</code></p>

<ul>
<li>When number of active tasks is lower than <code>number of executors * number of cores</code>, allocated YARN resources are not fully utilized.</li>
</ul>


<p><code>
executor.threadpool.activeTasks
</code></p>

<ul>
<li>How much RAM is used for RDD cache.</li>
</ul>


<p><code>
driver.BlockManager.memory.memUsed_MB
</code></p>

<ul>
<li>When there is not enough RAM for RDD cache, how much data has been spilled to disk.
You should increase executor memory or change <code>spark.memory.fraction</code> Spark property to avoid performance degradation.</li>
</ul>


<p><code>
driver.BlockManager.disk.diskSpaceUsed_MB
</code></p>

<ul>
<li>What is JVM memory utilization on Spark driver.</li>
</ul>


<p><code>
driver.jvm.heap.used
driver.jvm.non-heap.used
driver.jvm.pools.G1-Old-Gen.used
driver.jvm.pools.G1-Eden-Space.used
driver.jvm.pools.G1-Survivor-Space.used
</code></p>

<ul>
<li>How much time is spent on GC on Spark driver.</li>
</ul>


<p><code>
driver.jvm.G1-Old-Generation.time
driver.jvm.G1-Young-Generation.time
</code></p>

<ul>
<li>What is JMV memory utilization on Spark executors.</li>
</ul>


<p><code>
[0-9]*.jvm.heap.used
[0-9]*.jvm.non-heap.used
[0-9]*.jvm.pools.G1-Old-Gen.used
[0-9]*.jvm.pools.G1-Survivor-Space.used
[0-9]*.jvm.pools.G1-Eden-Space.used
</code></p>

<ul>
<li>How much time is spent on GC on Spark executors.</li>
</ul>


<p><code>
[0-9]*.jvm.G1-Old-Generation.time
[0-9]*.jvm.G1-Young-Generation.time
</code></p>

<h3>Grafana</h3>

<p>While you configure first Grafana dashboard for Spark application, the first problem pops up:</p>

<blockquote><p>How to configure Graphite query when metrics for every Spark application run are reported under its own application id?</p></blockquote>

<p>If you are lucky and brave enough to use Spark 2.1, pin the application metric into static application name:</p>

<p><code>
--conf spark.metrics.namespace=my_application_name
</code></p>

<p>For Spark versions older than 2.1, a few tricks with Graphite built-in functions are needed.</p>

<p>Driver metrics use wildcard <code>.*(application_[0-9]+).*</code>
and <code>aliasSub</code> Graphite function to present &lsquo;application id&rsquo; as graph legend:</p>

<p><code>
aliasSub(stats.analytics.$job_name.*.prod.$dc.*.driver.jvm.heap.used, ".*(application_[0-9]+).*", "heap: \1")
</code></p>

<p>For executor metrics again use wildcard <code>.*(application_[0-9]+).*</code>,
<code>groupByNode</code> Graphite function to sum metrics from all Spark executors
and finally <code>aliasSub</code> Graphite function to present &lsquo;application id&rsquo; as graph legend:</p>

<p><code>
aliasSub(groupByNode(stats.analytics.$job_name.*.prod.$dc.*.[0-9]*.jvm.heap.used, 6, "sumSeries"), "(.*)", "heap: \1")
</code></p>

<p>Finally Grafana dashboard for Spark Job might look like:</p>

<p><img src="/images/blog/spark_job_monitoring.png" title="[spark job monitoring]" ></p>

<p>If Spark application is restarted frequently, metrics for old, already finished runs should be deleted from Graphite.
Because Graphite does not compact inactive metrics, old metrics slow down Graphite itself and Grafana queries.</p>

<h2>Graceful stop</h2>

<p>The last puzzle element is how to stop Spark Streaming application deployed on YARN in a graceful way.
The standard method for stopping (or rather killing) YARN application is using a command <code>yarn application -kill [applicationId]</code>.
And this command stops the Spark Streaming application but this could happen in the middle of a batch.
So if the job reads data from Kafka, saves processing results on HDFS and finally commits Kafka offsets
you should expect duplicated data on HDFS when job was stopped just before committing offsets.</p>

<p>The first attempt to solve graceful shutdown issue was to call Spark streaming context stop method in a shutdown hook.</p>

<p>```scala
sys.addShutdownHook {</p>

<pre><code>streamingContext.stop(stopSparkContext = true, stopGracefully = true)
</code></pre>

<p>}
```</p>

<p>Disappointingly a shutdown hook is called too late to finish started batch and Spark application is killed almost immediately.
Moreover there is no guarantee that a shutdown hook will be called by JVM at all.</p>

<p>At the time of writing this blog post the only confirmed way to shutdown gracefully Spark Streaming application on YARN
is to notifying somehow the application about planned shutdown, and then stop streaming context programmatically (but not from shutdown hook).
Command <code>yarn application -kill</code> should be used only as a last resort if notified application did not stop after defined timeout.</p>

<p>The application can be notified about planned shutdown using marker file on HDFS (the easiest way),
or using simple Socket/HTTP endpoint exposed on the driver (sophisticated way).</p>

<p>Because I like KISS principle, below you can find shell script pseudo-code for starting / stopping Spark Streaming application using marker file:</p>

<p>```bash
start() {</p>

<pre><code>hdfs dfs -touchz /path/to/marker/my_job_unique_name
spark-submit ...
</code></pre>

<p>}</p>

<p>stop() {</p>

<pre><code>hdfs dfs -rm /path/to/marker/my_job_unique_name
force_kill=true
application_id=$(yarn application -list | grep -oe "application_[0-9]*_[0-9]*"`)
for i in `seq 1 10`; do
    application_status=$(yarn application -status ${application_id} | grep "State : \(RUNNING\|ACCEPTED\)")
    if [ -n "$application_status" ]; then
        sleep 60s
    else
        force_kill=false
        break
    fi
done
$force_kill &amp;&amp; yarn application -kill ${application_id}
</code></pre>

<p>}
```</p>

<p>In the Spark Streaming application, background thread should monitor marker file,
and when the file disappears stop the context calling <code>streamingContext.stop(stopSparkContext = true, stopGracefully = true)</code>.</p>

<h2>Summary</h2>

<p>As you could see, configuration for mission critical Spark Streaming application deployed on YARN is quite complex.
It has been long, tedious and iterative learning process of all presented techniques by a few very smart devs.
But at the end, long-running Spark Streaming applications deployed on highly utilized YARN cluster are extraordinarily stable.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Application Assembly for Cluster Deployments]]></title>
    <link href="http://mkuthan.github.io/blog/2016/03/11/spark-application-assembly/"/>
    <updated>2016-03-11T00:00:00+00:00</updated>
    <id>http://mkuthan.github.io/blog/2016/03/11/spark-application-assembly</id>
    <content type="html"><![CDATA[<p>When I tried to deploy my first Spark application on a YARN cluster,
I realized that there was no clear and concise instruction how to prepare the application for deployment.
This blog post could be treated as missing manual on how to build Spark application written in Scala to get deployable binary.</p>

<p>This blog post assumes that your Spark application is built with <a href="http://www.scala-sbt.org/">SBT</a>.
As long as SBT is a mainstream tool for building Scala applications the assumption seems legit.
Please ensure that your project is configured with at least SBT 0.13.6.
Open <code>project/build.properties</code> file, verify the version and update SBT if needed:</p>

<p><code>
sbt.version=0.13.11
</code></p>

<h2>SBT Assembly Plugin</h2>

<p>The <code>spark-submit</code> script is a convenient way to launch Spark application on the YARN or Mesos cluster.
However, due to distributed nature of the cluster the application has to be prepared as single Java ARchive (JAR).
This archive includes all classes from your project with all of its dependencies.
This application assembly can be prepared using <a href="https://github.com/sbt/sbt-assembly">SBT Assembly Plugin</a>.</p>

<p>To enable SBT Assembly Plugin, add the plugin dependency to the <code>project/plugins.sbt</code> file:</p>

<p><code>
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.1")
</code></p>

<p>This basic setup can be verified by calling <code>sbt assembly</code> command.
The final assembly location depend on the Scala version, application name and application version.
The build result could be assembled into <code>target/scala-2.11/myapp-assembly-1.0.jar</code> file.</p>

<p>You can configure many aspects of SBT Assembly Plugin like custom merge strategy
but I found that it is much easier to keep the defaults and follow the plugin conventions.
And what is even more important you don&rsquo;t have to change defaults to get correct, deployable application binary assembled by the plugin.</p>

<h2>Provided dependencies scope</h2>

<p>As long as cluster provides Spark classes at runtime, Spark dependencies must be excluded from the assembled JAR.
If not, you should expect weird errors from Java classloader during application startup.
Additional benefit of assembly without Spark dependencies is faster deployment.
Please remember that application assembly must be copied over the network to the location accessible by all cluster nodes (e.g: HDFS or S3).</p>

<p>Look at dependency section in your build file, it should look similar to the code snippet below:</p>

<p>```
val sparkVersion = &ldquo;1.6.0&rdquo;</p>

<p>&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-core&rdquo; % sparkVersion,
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-sql&rdquo; % sparkVersion,
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-hive&rdquo; % sparkVersion,
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-mlib&rdquo; % sparkVersion,
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-graphx&rdquo; % sparkVersion,
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-streaming&rdquo; % sparkVersion,
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-streaming-kafka&rdquo; % sparkVersion,
(&hellip;)
```</p>

<p>The list of the Spark dependencies is always project specific.
SQL, Hive, MLib, GraphX and Streaming extensions are defined only for reference.
All defined dependencies are required by local build to compile code and run
<a href="http://mkuthan.github.io/blog/2015/03/01/spark-unit-testing/">tests</a>.
So they could not be removed from the build definition in the ordinary way because it will break the build at all.</p>

<p>SBT Assembly Plugin comes with additional dependency scope &ldquo;provided&rdquo;.
The scope is very similar to <a href="https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html">Maven provided scope</a>.
The provided dependency will be part of compilation and test, but excluded from the application assembly.</p>

<p>To configure provided scope for Spark dependencies change the definition as follows:</p>

<p>```
val sparkVersion = &ldquo;1.6.0&rdquo;</p>

<p>&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-core&rdquo; % sparkVersion % &ldquo;provided&rdquo;,
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-sql&rdquo; % sparkVersion % &ldquo;provided&rdquo;,
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-hive&rdquo; % sparkVersion % &ldquo;provided&rdquo;,
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-mlib&rdquo; % sparkVersion % &ldquo;provided&rdquo;,
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-graphx&rdquo; % sparkVersion % &ldquo;provided&rdquo;,
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-streaming&rdquo; % sparkVersion % &ldquo;provided&rdquo;,
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-streaming-kafka&rdquo; % sparkVersion
  exclude(&ldquo;log4j&rdquo;, &ldquo;log4j&rdquo;)
  exclude(&ldquo;org.spark-project.spark&rdquo;, &ldquo;unused&rdquo;),
(&hellip;)
```</p>

<p>Careful readers should notice that &ldquo;spark-streaming-kafka&rdquo; dependency has not been listed and marked as &ldquo;provided&rdquo;.
It was done by purpose because integration with Kafka is not part of Spark distribution assembly
and has to be assembled into application JAR.
The exclusion rules for &ldquo;spark-streaming-kafka&rdquo; dependency will be discussed later.</p>

<p>Ok, but how to recognize which libraries are part of Spark distribution assembly?
There is no simple answer to this question.
Look for <code>spark-assembly-*-1.6.0.jar</code> file on the cluster classpath,
list the assembly content and verify what is included and what is not.
In the assembly on my cluster I found core, sql, hive, mlib, graphx and streaming classes are embedded but not integration with Kafka.</p>

<p><code>
$ tar -tzf spark-assembly-1.6.0.jar
META-INF/
META-INF/MANIFEST.MF
org/
org/apache/
org/apache/spark/
org/apache/spark/HeartbeatReceiver
(...)
org/apache/spark/ml/
org/apache/spark/ml/Pipeline$SharedReadWrite$$anonfun$2.class
org/apache/spark/ml/tuning/
(...)
org/apache/spark/sql/
org/apache/spark/sql/UDFRegistration$$anonfun$3.class
org/apache/spark/sql/SQLContext$$anonfun$range$2.class
(...)
reference.conf
META-INF/NOTICE
</code></p>

<h2>SBT run and run-main</h2>

<p>Provided dependency scope unfortunately breaks SBT <code>run</code> and <code>run-main</code> tasks.
Because provided dependencies are excluded from the runtime classpath, you should expect <code>ClassNotFoundException</code> during application startup on local machine.
To fix this issue, provided dependencies must be explicitly added to all SBT tasks used for local run, e.g.:</p>

<p><code>
run in Compile &lt;&lt;= Defaults.runTask(fullClasspath in Compile, mainClass in(Compile, run), runner in(Compile, run))
runMain in Compile &lt;&lt;= Defaults.runMainTask(fullClasspath in Compile, runner in(Compile, run))
</code></p>

<h2>How to exclude Log4j from application assembly?</h2>

<p>Without Spark classes the application assembly is quite lightweight.
But the assembly size might be reduced event more!</p>

<p>Let assume that your application requires some logging provider.
As long as Spark internally uses Log4j, it means that Log4j is already on the cluster classpath.
But you may say that there is much better API for Scala than origin Log4j &ndash; and you are totally right.</p>

<p>The snippet below configure excellent Typesafe (Lightbend nowadays) <a href="https://github.com/typesafehub/scala-logging">Scala Logging Library</a> dependency.</p>

<p>```
&ldquo;com.typesafe.scala-logging&rdquo; %% &ldquo;scala-logging&rdquo; % &ldquo;3.1.0&rdquo;,</p>

<p>&ldquo;org.slf4j&rdquo; % &ldquo;slf4j-api&rdquo; % &ldquo;1.7.10&rdquo;,
&ldquo;org.slf4j&rdquo; % &ldquo;slf4j-log4j12&rdquo; % &ldquo;1.7.10&rdquo; exclude(&ldquo;log4j&rdquo;, &ldquo;log4j&rdquo;),</p>

<p>&ldquo;log4j&rdquo; % &ldquo;log4j&rdquo; % &ldquo;1.2.17&rdquo; % &ldquo;provided&rdquo;,
```</p>

<p>Scala Logging is a thin wrapper for SLF4J implemented using Scala macros.
The &ldquo;slf4j-log4j12&rdquo; is a binding library between SLF4J API and Log4j logger provider.
Three layers of indirection but who cares :&ndash;)</p>

<p>There is also top-level dependency to Log4J defined with provided scope.
But this is not enough to get rid of Log4j classes from the application assembly.
Because Log4j is also a transitive dependency of &ldquo;slf4j-log4j12&rdquo; it must be explicitly excluded.
If not, SBT Assembly Plugin adds Log4j classes to the assembly even if top level &ldquo;log4j&rdquo; dependency is marked as &ldquo;provided&rdquo;.
Not very intuitive but SBT Assembly Plugin works this way.</p>

<p>Alternatively you could disable transitive dependencies for &ldquo;slf4j-log4j12&rdquo; at all.
It could be especially useful for libraries with many transitive dependencies which are expected to be on the cluster classpath.</p>

<p><code>
"org.slf4j" % "slf4j-log4j12" % "1.7.10" intransitive()
</code></p>

<h2>Spark Streaming Kafka dependency</h2>

<p>Now we are ready to define dependency to &ldquo;spark-streaming-kafka&rdquo;.
Because Spark integration with Kafka typically is not a part of Spark assembly,
it must be embedded into application assembly.
The artifact should not be defined within &ldquo;provided&rdquo; scope.</p>

<p>```
val sparkVersion = &ldquo;1.6.0&rdquo;</p>

<p>(&hellip;)
&ldquo;org.apache.spark&rdquo; %% &ldquo;spark-streaming-kafka&rdquo; % sparkVersion
  exclude(&ldquo;log4j&rdquo;, &ldquo;log4j&rdquo;)
  exclude(&ldquo;org.spark-project.spark&rdquo;, &ldquo;unused&rdquo;),
(&hellip;)
```</p>

<p>Again, &ldquo;log4j&rdquo; transitive dependency of Kafka needs to be explicitly excluded.
I also found that marker class from weird Spark &ldquo;unused&rdquo; artifact breaks default SBT Assembly Plugin merge strategy.
It is much easier to exclude this dependency than customize merge strategy of the plugin.</p>

<h2>Where is Guava?</h2>

<p>When you look at your project dependencies you could easily find Guava (version 14.0.1 for Spark 1.6.0).
Ok, Guava is an excellent library so you decide to use the library in your application.</p>

<p><em>WRONG!</em></p>

<p>Guava is on the classpath during compilation and tests but at runtime you will get &ldquo;ClassNotFoundException&rdquo; or method not found error.
First, Guava is shaded in Spark distribution assembly under <code>org/spark-project/guava</code> package and should not be used directly.
Second, there is a huge chance for outdated Guava library on the cluster classpath.
In CDH 5.3 distribution, the installed Guava version is 11.0.2 released on Feb 22, 2012 &ndash; more than 4 years ago!
Since the Guava is <a href="http://i.stack.imgur.com/8K6N8.jpg">binary compatible</a> only between 2 or 3 latest major releases it is a real blocker.</p>

<p>There are experimental configuration flags for Spark <code>spark.driver.userClassPathFirst</code> and <code>spark.executor.userClassPathFirst</code>.
In theory it gives user-added jars precedence over Spark&rsquo;s own jars when loading classes in the the driver.
But in practice it does not work, at least for me :&ndash;(.</p>

<p>In general you should avoid external dependencies at all cost when you develop application deployed on the YARN cluster.
Classloader hell is even bigger than in JEE containers like JBoss or WebLogic.
Look for the libraries with minimal transitive dependencies and narrowed features.
For example, if you need a cache, choose <a href="https://github.com/ben-manes/caffeine">Caffeine</a> over Guava.</p>

<h2>Deployment optimization for YARN cluster</h2>

<p>When application is deployed on YARN cluster using <code>spark-submit</code> script,
the script upload Spark distribution assembly to the cluster during every deployment.
The distribution assembly size is over 100MB, ten times more than typical application assembly!</p>

<p>So I really recommend to install Spark distribution assembly on well known location on the cluster
and define <code>spark.yarn.jar</code> property for <code>spark-submit</code>.
The assembly will not be copied over the network during every deployment.</p>

<p><code>
spark.yarn.jar=hdfs:///apps/spark/assembly/spark-assembly-1.6.0.jar
</code></p>

<h2>Summary</h2>

<p>I witnessed a few Spark projects where <code>build.sbt</code> were more complex than application itself.
And application assembly was bloated with unnecessary 3rd party classes and deployment process took ages.
Build configuration described in this blog post should help you deploy Spark application on the cluster smoothly
and still keep SBT configuration easy to maintain.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark and Kafka Integration Patterns, Part 2]]></title>
    <link href="http://mkuthan.github.io/blog/2016/01/29/spark-kafka-integration2/"/>
    <updated>2016-01-29T00:00:00+00:00</updated>
    <id>http://mkuthan.github.io/blog/2016/01/29/spark-kafka-integration2</id>
    <content type="html"><![CDATA[<p>In the <a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">world beyond batch</a>,
streaming data processing is a future of dig data.
Despite of the streaming framework using for data processing, tight integration with replayable data source like Apache Kafka is often required.
The streaming applications often use Apache Kafka as a data source, or as a destination for processing results.</p>

<p>Apache Spark distribution has built-in support for reading from Kafka, but surprisingly does not offer any
integration for sending processing result back to Kafka.
This blog post aims to fill this gap in the Spark ecosystem.</p>

<p>In the <a href="http://mkuthan.github.io/blog/2015/08/06/spark-kafka-integration1/">first part</a> of the series
you learned how to manage Kafka producer using Scala lazy evaluation feature
and how to reuse single Kafka producer instance on Spark executor.</p>

<p>In this blog post you will learn how to publish stream processing results to Apache Kafka in reliable way.
First you will learn how Kafka Producer is working,
how to configure Kafka producer and how to setup Kafka cluster to achieve desired reliability.
In the second part of the blog post,
I will present how to implement convenient library for sending continuous sequence of RDDs
(<a href="https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.streaming.dstream.DStream">DStream</a>)
to Apache Kafka topic, as easy as in the code snippet below.</p>

<p>``` scala
// enable implicit conversions
import KafkaDStreamSink._</p>

<p>// send dstream to Kafka
dstream.sendToKafka(kafkaProducerConfig, topic)
```</p>

<h2>Quick introduction to Kafka</h2>

<p>Kafka is a distributed, partitioned, replicated message broker.
Basic architecture knowledge is a prerequisite to understand Spark and Kafka integration challenges.
You can safely skip this section, if you are already familiar with Kafka concepts.</p>

<p>For convenience I copied essential terminology definitions directly from Kafka
<a href="http://kafka.apache.org/documentation.html#introduction">documentation</a>:</p>

<ul>
<li>Kafka maintains feeds of messages in categories called topics.</li>
<li>We&rsquo;ll call processes that publish messages to a Kafka topic producers.</li>
<li>We&rsquo;ll call processes that subscribe to topics and process the feed of published messages consumers.</li>
<li>Kafka is run as a cluster comprised of one or more servers each of which is called a broker.</li>
</ul>


<p>So, at a high level, producers send messages over the network to the Kafka cluster which in turn serves them up to consumers like this:</p>

<p><img class="<a" src="href="http://kafka.apache.org/images/producer_consumer.png">http://kafka.apache.org/images/producer_consumer.png</a>"></p>

<p>This is a bare minimum you have to know but I really encourage you to read Kafka reference manual thoroughly.</p>

<h2>Kafka producer API</h2>

<p>First we need to know how Kafka producer is working.
Kafka producer exposes very simple API for sending messages to Kafka topics.
The most important methods from <code>KafkaProducer</code> class are listed below:</p>

<p><code>java KafkaProducer API
j.u.c.Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K,V&gt; record)
j.u.c.Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K,V&gt; record, Callback callback)
void flush()
void close()
</code></p>

<p>The <code>send()</code> methods asynchronously send a key-value record to a topic and will return immediately once the record has been stored in the buffer of records waiting to be sent.
This kind of API is not very convenient for developers, but is crucial to achieve high throughput and low latency.</p>

<p>If you want to ensure that request has been completed, you can invoke blocking <code>get()</code> on the future returned by the <code>send()</code> methods.
The main drawback of calling <code>get()</code> is a huge performance penalty because it disables batching effectively.
You can not expect high throughput and low latency if the execution is blocked on every message and every single message needs to be sent separately.</p>

<p>Fully non-blocking usage requires use of the callback. The callback will be invoked when the request is complete.
Note that callback is executed in Kafka producer I/O thread so should not block the caller, the callback must be as lightweight as possible.
The callback must be also properly synchronized due to <a href="https://en.wikipedia.org/wiki/Java_memory_model">Java memory model</a>.</p>

<p>If the Kafka producer caller does not check result of the <code>send()</code> method using future or callback,
it means that if Kafka producer crashed all messages from the internal Kafka producer buffer will be lost.
This is the first, very important element of any integration library with Kafka,
we should expect callback handling to avoid data lost and achieve good performance.</p>

<p>The <code>flush()</code> method makes all buffered messages ready to send, and blocks on the completion of the requests associated with these messages.
The <code>close()</code> method is like the <code>flush()</code> method but also closes the producer.</p>

<p>The <code>flush()</code> method could be very handy if the Streaming framework wants to ensure that all messages have been sent before processing next part of the stream.
With <code>flush()</code> method streaming framework is able to flush the messages to Kafka brokers to simulate commit behaviour.</p>

<p>Method <code>flush()</code> was added in Kafka 0.9 release (<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-8+-+Add+a+flush+method+to+the+producer+API">KIP-8</a>).
Before Kafka 0.9, the only safe and straightforward way to flush messages from Kafka producer internal buffer was to close the producer.</p>

<h2>Kafka configuration</h2>

<p>If the message must be reliable published on Kafka cluster, Kafka producer and Kafka cluster needs to be configured with care.
It needs to be done independently of chosen streaming framework.</p>

<p>Kafka producer buffers messages in memory before sending.
When our memory buffer is exhausted, Kafka producer must either stop accepting new records (block) or throw errors.
By default Kafka producer blocks and this behavior is legitimate for stream processing.
The processing should be delayed if Kafka producer memory buffer is full and could not accept new messages.
Ensure that <code>block.on.buffer.full</code> Kafka producer configuration property is set.</p>

<p>With default configuration, when Kafka broker (leader of the partition) receive the message, store the message in memory and immediately send acknowledgment to Kafka producer.
To avoid data loss the message should be replicated to at least one replica (follower).
Only when the follower acknowledges the leader, the leader acknowledges the producer.</p>

<p>This guarantee you will get with <code>ack=all</code> property in Kafka producer configuration.
This guarantees that the record will not be lost as long as at least one in-sync replica remains alive.</p>

<p>But this is not enough. The minimum number of replicas in-sync must be defined.
You should configure <code>min.insync.replicas</code> property for every topic.
I recommend to configure at least 2 in-sync replicas (leader and one follower).
If you have datacenter with two zones, I also recommend to keep leader in the first zone and 2 followers in the second zone.
This configuration guarantees that every message will be stored in both zones.</p>

<p>We are almost done with Kafka cluster configuration.
When you set <code>min.insync.replicas=2</code> property, the topic should be replicated with factor 2 + N.
Where N is the number of brokers which could fail, and Kafka producer will still be able to publish messages to the cluster.
I recommend to configure replication factor 3 for the topic (or more).</p>

<p>With replication factor 3, the number of brokers in the cluster should be at least 3 + M.
When one or more brokers are unavailable, you will get underreplicated partitions state of the topics.
With more brokers in the cluster than replication factor, you can reassign underreplicated partitions and achieve fully replicated cluster again.
I recommend to build the 4 nodes cluster at least for topics with replication factor 3.</p>

<p>The last important Kafka cluster configuration property is <code>unclean.leader.election.enable</code>.
It should be disabled (by default it is enabled) to avoid unrecoverable exceptions from Kafka consumer.
Consider the situation when the latest committed offset is N,
but after leader failure, the latest offset on the new leader is M &lt; N.
M &lt; N because the new leader was elected from the lagging follower (not in-sync replica).
When the streaming engine ask for data from offset N using Kafka consumer, it will get an exception because the offset N does not exist yet.
Someone will have to fix offsets manually.</p>

<p>So the minimal recommended Kafka setup for reliable message processing is:</p>

<ul>
<li>4 nodes in the cluster</li>
<li><code>unclean.leader.election.enable=false</code> in the brokers configuration</li>
<li>replication factor for the topics &ndash; 3</li>
<li><code>min.insync.replicas=2</code> property in topic configuration</li>
<li><code>ack=all</code> property in the producer configuration</li>
<li><code>block.on.buffer.full=true</code> property in the producer configuration</li>
</ul>


<p>With the above setup your configuration should be resistant to single broker failure,
and Kafka consumers will survive new leader election.</p>

<p>You could also take look at <code>replica.lag.max.messages</code> and <code>replica.lag.time.max.ms</code> properties
for tuning when the follower is removed from ISR by the leader.
But this is out of this blog post scope.</p>

<h2>How to expand Spark API?</h2>

<p>After this not so short introduction, we are ready to disassembly
<a href="https://github.com/mkuthan/example-spark-kafka">integration library</a> for Spark Streaming and Apache Kafka.
First <code>DStream</code> needs to be somehow expanded to support new method <code>sendToKafka()</code>.</p>

<p><code>scala
dstream.sendToKafka(kafkaProducerConfig, topic)
</code></p>

<p>In Scala, the only way to add methods to existing API, is to use an implicit conversion feature.</p>

<p>``` scala
object KafkaDStreamSink {</p>

<p>  import scala.language.implicitConversions</p>

<p>  implicit def createKafkaDStreamSink(dstream: DStream[KafkaPayload]): KafkaDStreamSink = {</p>

<pre><code>new KafkaDStreamSink(dstream)
</code></pre>

<p>  }</p>

<p>}
```</p>

<p>Whenever Scala compiler finds call to non-existing method <code>sendToKafka()</code> on <code>DStream</code> class,
the stream will be implicitly wrapped into <code>KafkaDStreamSink</code> class,
where method <code>sendToKafka</code> is finally defined.
To enable implicit conversion for <code>DStream</code> add the import statement to your code, that&rsquo;s all.</p>

<p><code>scala
import KafkaDStreamSink._
</code></p>

<h2>How to send to Kafka in reliable way?</h2>

<p>Let&rsquo;s check how <code>sendToKafka()</code> method is defined step by step, this is the core part of the integration library.</p>

<p>``` scala
class KafkaDStreamSink(dstream: DStream[KafkaPayload]) {</p>

<p>  def sendToKafka(config: Map[String, String], topic: String): Unit = {</p>

<pre><code>dstream.foreachRDD { rdd =&gt;
  rdd.foreachPartition { records =&gt;
    // send records from every partition to Kafka
  }
}
</code></pre>

<p>  }
```</p>

<p>There are two loops, first on wrapped <code>dstream</code> and second on <code>rdd</code> for every partition.
Quite standard pattern for Spark programming model.
Records from every partition are ready to be sent to Kafka topic by Spark executors.
The destination topic name is given explicitly as the last parameter of the <code>sendToKafka()</code> method.</p>

<p>First step in the inner loop is getting Kafka producer instance from the <code>KafkaProducerFactory</code>.</p>

<p><code>scala
rdd.foreachPartition { records =&gt;
  val producer = KafkaProducerFactory.getOrCreateProducer(config)
  (...)
</code></p>

<p>The factory creates only single instance of the producer for any given producer configuration.
If the producer instance has been already created, the existing instance is returned and reused.
Kafka producer caching is crucial for the performance reasons,
because establishing a connection to the cluster takes time.
It is a much more time consuming operation than opening plain socket connection,
as Kafka producer needs to discover leaders for all partitions.
Please refer to <a href="http://mkuthan.github.io/blog/2015/08/06/spark-kafka-integration1/">first part</a> of this blog post
and <code>KafkaProducerFactory</code>
<a href="https://github.com/mkuthan/example-spark-kafka/blob/master/src/main/scala/org/mkuthan/spark/KafkaProducerFactory.scala">source</a>
for more details about the factory implementation.</p>

<p>For debugging purposes logger and Spark task context are needed.</p>

<p>``` scala
import org.apache.spark.TaskContext
import org.slf4j.LoggerFactory
(&hellip;)</p>

<p>rdd.foreachPartition { records =>
  val producer = KafkaProducerFactory.getOrCreateProducer(config)</p>

<p>  val context = TaskContext.get
  val logger = Logger(LoggerFactory.getLogger(classOf[KafkaDStreamSink]))
  (&hellip;)
```</p>

<p>You could use any logging framework but the logger itself has to be defined in the foreachPartition loop
to avoid weird serialization issues.
Spark task context will be used to get current partition identifier.
I don&rsquo;t like static call for getting task context, but this is an official way to do that.
See pull request <a href="https://github.com/apache/spark/pull/5927">SPARK-5927</a> for more details.</p>

<p>Before we go further, Kafka producer callback for error handling needs to be introduced.</p>

<p>``` scala KafkaDStreamSinkExceptionHandler
import java.util.concurrent.atomic.AtomicReference
import org.apache.kafka.clients.producer.Callback</p>

<p>class KafkaDStreamSinkExceptionHandler extends Callback {</p>

<p>  private val lastException = new AtomicReference<a href="None">Option[Exception]</a></p>

<p>  override def onCompletion(metadata: RecordMetadata, exception: Exception): Unit =</p>

<pre><code>lastException.set(Option(exception))
</code></pre>

<p>  def throwExceptionIfAny(): Unit =</p>

<pre><code>lastException.getAndSet(None).foreach(ex =&gt; throw ex)
</code></pre>

<p>}
```</p>

<p>Method <code>onCompletion()</code> of the callback is called when the message sent to the Kafka cluster has been acknowledged.
Exactly one of the callback arguments will be non-null, <code>metadata</code> or <code>exception</code>.
<code>KafkaDStreamSinkExceptionHandler</code> class keeps last exception registered by the callback (if any).
The client of the callback is able to rethrow registered exception using <code>throwExceptionIfAny()</code> method.
Because <code>onCompletion()</code> and <code>throwExceptionIfAny()</code> methods are called from different threads,
last exception has to be kept in thread-safe data structure <code>AtomicReference</code>.</p>

<p>Finally we are ready to send records to Kafka using created callback.</p>

<p>``` scala
rdd.foreachPartition { records =>
  val producer = KafkaProducerFactory.getOrCreateProducer(config)</p>

<p>  val context = TaskContext.get
  val logger = Logger(LoggerFactory.getLogger(classOf[KafkaDStreamSink]))</p>

<p>  val callback = new KafkaDStreamSinkExceptionHandler</p>

<p>  logger.debug(s"Send Spark partition: ${context.partitionId} to Kafka topic: $topic")
  val metadata = records.map { record =></p>

<pre><code>callback.throwExceptionIfAny()
producer.send(new ProducerRecord(topic, record.key.orNull, record.value), callback)
</code></pre>

<p>  }.toList
```</p>

<p>First the callback is examined for registered exception.
If one of the previous record could not be sent, the exception is propagated to Spark framework.
If any redelivery policy is needed it should be configured on Kafka producer level.
Look at Kafka producer configuration properties <code>retries</code> and <code>retry.backoff.ms</code>.
Finally Kafka producer metadata are collected and materialized by calling <code>toList()</code> method.
At this moment, Kafka producer starts sending records in background I/O thread.
To achieve high throughput Kafka producer sends records in batches.</p>

<p>Because we want to achieve natural back pressure for our stream processing,
next batch needs to be blocked until records from current batch are really acknowledged by the Kafka brokers.
So for each collected metadata (Java <code>j.u.c.Future</code>), method <code>get()</code> is called to ensure that record has been sent to the brokers.</p>

<p>``` scala
rdd.foreachPartition { records =>
  val producer = KafkaProducerFactory.getOrCreateProducer(config)</p>

<p>  val context = TaskContext.get
  val logger = Logger(LoggerFactory.getLogger(classOf[KafkaDStreamSink]))</p>

<p>  val callback = new KafkaDStreamSinkExceptionHandler</p>

<p>  logger.debug(s"Send Spark partition: ${context.partitionId} to Kafka topic: $topic")
  val metadata = records.map { record =></p>

<pre><code>callback.throwExceptionIfAny()
producer.send(new ProducerRecord(topic, record.key.orNull, record.value), callback)
</code></pre>

<p>  }.toList</p>

<p>  logger.debug(s"Flush Spark partition: ${context.partitionId} to Kafka topic: $topic")
  metadata.foreach { metadata => metadata.get() }
```</p>

<p>As long as records sending was started moment ago, it is likelihood that records have been already sent
and <code>get()</code> method does not block.
However if the <code>get()</code> call is blocked, it means that there are unsent messages in the internal Kafka producer buffer
and the processing should be blocked as well.</p>

<p>Finally <code>sendToKafka()</code> method should propagate exception recorded by the callback (if any).
Complete method is presented below for reference.</p>

<p>``` scala sendToKafka
def sendToKafka(config: Map[String, String], topic: String): Unit = {
  dstream.foreachRDD { rdd =></p>

<pre><code>rdd.foreachPartition { records =&gt;
  val producer = KafkaProducerFactory.getOrCreateProducer(config)

  val context = TaskContext.get
  val logger = Logger(LoggerFactory.getLogger(classOf[KafkaDStreamSink]))

  val callback = new KafkaDStreamSinkExceptionHandler

  logger.debug(s"Send Spark partition: ${context.partitionId} to Kafka topic: $topic")
  val metadata = records.map { record =&gt;
    callback.throwExceptionIfAny()
    producer.send(new ProducerRecord(topic, record.key.orNull, record.value), callback)
  }.toList

  logger.debug(s"Flush Spark partition: ${context.partitionId} to Kafka topic: $topic")
  metadata.foreach { metadata =&gt; metadata.get() }

  callback.throwExceptionIfAny()
}
</code></pre>

<p>  }
}
```</p>

<p>The method is not very complex but there are a few important elements
if you don&rsquo;t want to lose processing results and if you need back pressure mechanism:</p>

<ul>
<li>Method <code>sendToKafka()</code> should fail fast if record could not be sent to Kafka. Don&rsquo;t worry Spark will execute failed task again.</li>
<li>Method <code>sendToKafka()</code> should block Spark processing if Kafka producer slows down.</li>
<li>Method <code>sendToKafka()</code> should flush records buffered by Kafka producer explicitly, to avoid data loss.</li>
<li>Kafka producer needs to be reused by Spark executor to avoid connection to Kafka overhead.</li>
<li>Kafka producer needs to be explicitly closed when Spark shutdowns executors to avoid data loss.</li>
</ul>


<h2>Summary</h2>

<p>The complete, working project is published on
<a href="https://github.com/mkuthan/example-spark-kafka">https://github.com/mkuthan/example-spark-kafka</a>.
You can clone/fork the project and do some experiments by yourself.</p>

<p>There is also alternative library developed by Cloudera
<a href="https://github.com/cloudera/spark-kafka-writer">spark-kafka-writer</a>
emerged from closed pull request <a href="https://github.com/apache/spark/pull/2994">SPARK-2994</a>.
Unfortunately at the time of this writing,
the library used obsolete Scala Kafka producer API and did not send processing results in reliable way.</p>

<p>I hope that some day we will find reliable, mature library for sending processing result to Apache Kafka
in the official Spark distribution.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark and Kafka Integration Patterns, Part 1]]></title>
    <link href="http://mkuthan.github.io/blog/2015/08/06/spark-kafka-integration1/"/>
    <updated>2015-08-06T00:00:00+00:00</updated>
    <id>http://mkuthan.github.io/blog/2015/08/06/spark-kafka-integration1</id>
    <content type="html"><![CDATA[<p>I published post on the <a href="http://allegro.tech/">allegro.tech</a> blog, how to integrate Spark Streaming and Kafka.
In the blog post you will find how to avoid <code>java.io.NotSerializableException</code> exception
when Kafka producer is used for publishing results of the Spark Streaming processing.</p>

<p><a href="http://allegro.tech/2015/08/spark-kafka-integration.html">http://allegro.tech/spark-kafka-integration.html</a></p>

<p>You could be also interested in the
<a href="http://mkuthan.github.io/blog/2016/01/29/spark-kafka-integration2/">following part</a> of this blog post where
I presented complete library for sending Spark Streaming processing results to Kafka.</p>

<p>Happy reading :&ndash;)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark and Spark Streaming Unit Testing]]></title>
    <link href="http://mkuthan.github.io/blog/2015/03/01/spark-unit-testing/"/>
    <updated>2015-03-01T00:00:00+00:00</updated>
    <id>http://mkuthan.github.io/blog/2015/03/01/spark-unit-testing</id>
    <content type="html"><![CDATA[<p>When you develop distributed system, it is crucial to make it easy to test.
Execute tests in controlled environment, ideally from your IDE.
Long develop-test-develop cycle for complex systems could kill your productivity.
Below you find my testing strategy for Spark and Spark Streaming applications.</p>

<h2>Unit or integration tests, that is the question</h2>

<p>Our hypothetical Spark application pulls data from Apache Kafka, apply transformations using
<a href="https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.rdd.RDD">RDDs</a>
and
<a href="https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.streaming.dstream.DStream">DStreams</a>
and persist outcomes into Cassandra or Elastic Search database.
On production Spark application is deployed on YARN or Mesos cluster, and everything is glued with ZooKeeper.
Big picture of the stream processing architecture is presented below:</p>

<p><img class="<a" src="href="http://yuml.me/diagram/scruffy;dir:LR/class/">http://yuml.me/diagram/scruffy;dir:LR/class/</a>[Apache%20Kafka]&ndash;[Spark%20Streaming],[Spark%20Streaming]&ndash;[Cassandra],[Spark%20Streaming]&ndash;[Elastic%20Search],[Spark%20Streaming],[Zookeeper{bg:cornsilk}],[YARN%20or%20Mesos%20Cluster{bg:cornsilk}]"></p>

<p>Lots of moving parts, not so easy to configure and test.
Even with automated provisioning implemented with Vagrant, Docker and Ansible.
If you can&rsquo;t test everything, test at least the most important part of your application &ndash; transformations &ndash; implemented with Spark.</p>

<p>Spark claims, that it is friendly to unit testing with any popular unit test framework.
To be strict, Spark supports rather lightweight integration testing, not unit testing, IMHO.
But still it is much more convenient to test transformation logic locally, than deploying all parts on YARN.</p>

<p>There is a pull request <a href="https://github.com/apache/spark/pull/1751">SPARK-1751</a> that adds &ldquo;unit tests&rdquo; support for Apache Kafka streams.
Should we follow that way? Embedded ZooKeeper and embedded Apache Kafka are needed, the test fixture is complex and cumbersome.
Perhaps tests would be fragile and hard to maintain. This approach makes sense for Spark core team, they want to test Spark and Kafka integration.</p>

<h2>What should be tested?</h2>

<p>Our transformation logic implemented with Spark, nothing more. But how to test the logic so tightly coupled to Spark API (RDD, DStream)?
Let&rsquo;s define how typical Spark application is organized. Our hypothetical application structure looks like this:</p>

<ol>
<li>Initialize <code>SparkContext</code> or <code>StreamingContext</code>.</li>
<li>Create RDD or DStream for given source (e.g: Apache Kafka)</li>
<li>Evaluate transformations on RDD or DStream API.</li>
<li>Put transformation outcomes (e.g: aggregations) into external database.</li>
</ol>


<h3>Context</h3>

<p><code>SparkContext</code> and <code>StreamingContext</code> could be easily initialized for testing purposes.
Set master URL to <code>local</code>, run the operations and then stop context gracefully.</p>

<p>``` scala SparkContext Initialization
class SparkExampleSpec extends FlatSpec with BeforeAndAfter {</p>

<p>  private val master = &ldquo;local[2]&rdquo;
  private val appName = &ldquo;example-spark&rdquo;</p>

<p>  private var sc: SparkContext = _</p>

<p>  before {</p>

<pre><code>val conf = new SparkConf()
  .setMaster(master)
  .setAppName(appName)

sc = new SparkContext(conf)
</code></pre>

<p>  }</p>

<p>  after {</p>

<pre><code>if (sc != null) {
  sc.stop()
}
</code></pre>

<p>  }
  (&hellip;)</p>

<p>```</p>

<p>``` scala StreamingContext Initialization
class SparkStreamingExampleSpec extends FlatSpec with BeforeAndAfter {</p>

<p>  private val master = &ldquo;local[2]&rdquo;
  private val appName = &ldquo;example-spark-streaming&rdquo;
  private val batchDuration = Seconds(1)
  private val checkpointDir = Files.createTempDirectory(appName).toString</p>

<p>  private var sc: SparkContext = _
  private var ssc: StreamingContext = _</p>

<p>  before {</p>

<pre><code>val conf = new SparkConf()
  .setMaster(master)
  .setAppName(appName)

ssc = new StreamingContext(conf, batchDuration)
ssc.checkpoint(checkpointDir)

sc = ssc.sparkContext
</code></pre>

<p>  }</p>

<p>  after {</p>

<pre><code>if (ssc != null) {
  ssc.stop()
}
</code></pre>

<p>  }</p>

<p>  (&hellip;)
```</p>

<h3>RDD and DStream</h3>

<p>The problematic part is how to create RDD or DStream.
For testing purposes it must be simplified to avoid embedded Kafka and ZooKeeper.
Below you can find examples how to create in-memory RDD and DStream.</p>

<p><code>scala In-memory RDD
val lines = Seq("To be or not to be.", "That is the question.")
val rdd = sparkContext.parallelize(lines)
</code></p>

<p>``` scala In-memory DStream
val lines = mutable.Queue<a href="">RDD[String]</a>
val dstream = streamingContext.queueStream(lines)</p>

<p>// append data to DStream
lines += sparkContext.makeRDD(Seq(&ldquo;To be or not to be.&rdquo;, &ldquo;That is the question.&rdquo;))
```</p>

<h3>Transformation logic</h3>

<p>The most important part of our application &ndash; transformations logic &ndash; must be encapsulated in separate class or object.
Object is preferred to avoid class serialization overhead. Exactly the same code is used by the application and by the test.</p>

<p>``` scala WordCount.scala
case class WordCount(word: String, count: Int)</p>

<p>object WordCount {
  def count(lines: RDD[String], stopWords: Set[String]): RDD[WordCount] = {</p>

<pre><code>val words = lines.flatMap(_.split("\\s"))
  .map(_.strip(",").strip(".").toLowerCase)
  .filter(!stopWords.contains(_)).filter(!_.isEmpty)

val wordCounts = words.map(word =&gt; (word, 1)).reduceByKey(_ + _).map {
  case (word: String, count: Int) =&gt; WordCount(word, count)
}

val sortedWordCounts = wordCounts.sortBy(_.word)

sortedWordCounts
</code></pre>

<p>  }
}
```</p>

<h2>Spark test</h2>

<p>Now it is time to implement our first test for WordCount transformation.
The code of test is very straightforward and easy to read.
Single point of truth, the best documentation of your system, always up-to-date.</p>

<p>``` scala
&ldquo;Shakespeare most famous quote&rdquo; should &ldquo;be counted&rdquo; in {</p>

<pre><code>Given("quote")
val lines = Array("To be or not to be.", "That is the question.")

Given("stop words")
val stopWords = Set("the")

When("count words")
val wordCounts = WordCount.count(sc.parallelize(lines), stopWords).collect()

Then("words counted")
wordCounts should equal(Array(
  WordCount("be", 2),
  WordCount("is", 1),
  WordCount("not", 1),
  WordCount("or", 1),
  WordCount("question", 1),
  WordCount("that", 1),
  WordCount("to", 2)))
</code></pre>

<p>  }
```</p>

<h2>Spark Streaming test</h2>

<p>Spark Streaming transformations are much more complex to test.
The full control over clock is needed to manually manage batches, slides and windows.
Without controlled clock you would end up with complex tests with many <code>Thread.sleeep</code> calls.
And the test execution would take ages.
The only downside is that you will not have extra time for coffee during tests execution.</p>

<p>Spark Streaming provides necessary abstraction over system clock, <code>ManualClock</code> class.
Unfortunately <code>ManualClock</code> class is declared as package private. Some hack is needed.
The wrapper presented below, is an adapter for the original <code>ManualClock</code> class but without access restriction.</p>

<p>``` scala ClockWrapper.scala
package org.apache.spark.streaming</p>

<p>import org.apache.spark.streaming.util.ManualClock</p>

<p>class ClockWrapper(ssc: StreamingContext) {</p>

<p>  def getTimeMillis(): Long = manualClock().currentTime()</p>

<p>  def setTime(timeToSet: Long) = manualClock().setTime(timeToSet)</p>

<p>  def advance(timeToAdd: Long) = manualClock().addToTime(timeToAdd)</p>

<p>  def waitTillTime(targetTime: Long): Long = manualClock().waitTillTime(targetTime)</p>

<p>  private def manualClock(): ManualClock = {</p>

<pre><code>ssc.scheduler.clock.asInstanceOf[ManualClock]
</code></pre>

<p>  }</p>

<p>}
```</p>

<p>Now Spark Streaming test can be implemented in efficient way.
The test does not have to wait for system clock and test is implemented with millisecond precision.
You can easily test your windowed scenario from the very beginning to very end.
With given\when\then structure you should be able to understand tested logic without further explanations.</p>

<p>``` scala
&ldquo;Sample set&rdquo; should &ldquo;be counted&rdquo; in {
  Given(&ldquo;streaming context is initialized&rdquo;)
  val lines = mutable.Queue<a href="">RDD[String]</a></p>

<p>  var results = ListBuffer.empty[Array[WordCount]]</p>

<p>  WordCount.count(ssc.queueStream(lines), windowDuration, slideDuration) { (wordsCount: RDD[WordCount], time: Time) =></p>

<pre><code>results += wordsCount.collect()
</code></pre>

<p>  }</p>

<p>  ssc.start()</p>

<p>  When(&ldquo;first set of words queued&rdquo;)
  lines += sc.makeRDD(Seq(&ldquo;a&rdquo;, &ldquo;b&rdquo;))</p>

<p>  Then(&ldquo;words counted after first slide&rdquo;)
  clock.advance(slideDuration.milliseconds)
  eventually(timeout(1 second)) {</p>

<pre><code>results.last should equal(Array(
  WordCount("a", 1),
  WordCount("b", 1)))
</code></pre>

<p>  }</p>

<p>  When(&ldquo;second set of words queued&rdquo;)
  lines += sc.makeRDD(Seq(&ldquo;b&rdquo;, &ldquo;c&rdquo;))</p>

<p>  Then(&ldquo;words counted after second slide&rdquo;)
  clock.advance(slideDuration.milliseconds)
  eventually(timeout(1 second)) {</p>

<pre><code>results.last should equal(Array(
  WordCount("a", 1),
  WordCount("b", 2),
  WordCount("c", 1)))
</code></pre>

<p>  }</p>

<p>  When(&ldquo;nothing more queued&rdquo;)</p>

<p>  Then(&ldquo;word counted after third slide&rdquo;)
  clock.advance(slideDuration.milliseconds)
  eventually(timeout(1 second)) {</p>

<pre><code>results.last should equal(Array(
  WordCount("a", 0),
  WordCount("b", 1),
  WordCount("c", 1)))
</code></pre>

<p>  }</p>

<p>  When(&ldquo;nothing more queued&rdquo;)</p>

<p>  Then(&ldquo;word counted after fourth slide&rdquo;)
  clock.advance(slideDuration.milliseconds)
  eventually(timeout(1 second)) {</p>

<pre><code>results.last should equal(Array(
  WordCount("a", 0),
  WordCount("b", 0),
  WordCount("c", 0)))
</code></pre>

<p>  }
}
```</p>

<p>One comment to <code>Eventually</code> trait usage.
The trait is needed because Spark Streaming is a multithreaded application, and results are not computed immediately.
I found that 1 second timeout is enough for Spark Streaming to calculate the results.
The timeout is not related to batch, slide or window duration.</p>

<h2>Summary</h2>

<p>The complete, working project is published on <a href="https://github.com/mkuthan/example-spark">GitHub</a>.
You can clone/fork the project and do some experiments by yourself.</p>

<p>I hope that Spark committers expose <code>ManualClock</code> for others, eventually.
Control of time is necessary for efficient Spark Streaming application testing.</p>
]]></content>
  </entry>
  
</feed>
