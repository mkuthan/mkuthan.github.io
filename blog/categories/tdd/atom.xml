<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: TDD | Passionate Developer]]></title>
  <link href="http://mkuthan.github.io/blog/categories/tdd/atom.xml" rel="self"/>
  <link href="http://mkuthan.github.io/"/>
  <updated>2017-11-26T21:35:57+00:00</updated>
  <id>http://mkuthan.github.io/</id>
  <author>
    <name><![CDATA[Marcin Kuthan]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spark and Spark Streaming Unit Testing]]></title>
    <link href="http://mkuthan.github.io/blog/2015/03/01/spark-unit-testing/"/>
    <updated>2015-03-01T00:00:00+00:00</updated>
    <id>http://mkuthan.github.io/blog/2015/03/01/spark-unit-testing</id>
    <content type="html"><![CDATA[<p>When you develop distributed system, it is crucial to make it easy to test.
Execute tests in controlled environment, ideally from your IDE.
Long develop-test-develop cycle for complex systems could kill your productivity.
Below you find my testing strategy for Spark and Spark Streaming applications.</p>

<h2>Unit or integration tests, that is the question</h2>

<p>Our hypothetical Spark application pulls data from Apache Kafka, apply transformations using
<a href="https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.rdd.RDD">RDDs</a>
and
<a href="https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.streaming.dstream.DStream">DStreams</a>
and persist outcomes into Cassandra or Elastic Search database.
On production Spark application is deployed on YARN or Mesos cluster, and everything is glued with ZooKeeper.
Big picture of the stream processing architecture is presented below:</p>

<p><img class="<a" src="href="http://yuml.me/diagram/scruffy;dir:LR/class/">http://yuml.me/diagram/scruffy;dir:LR/class/</a>[Apache%20Kafka]&ndash;[Spark%20Streaming],[Spark%20Streaming]&ndash;[Cassandra],[Spark%20Streaming]&ndash;[Elastic%20Search],[Spark%20Streaming],[Zookeeper{bg:cornsilk}],[YARN%20or%20Mesos%20Cluster{bg:cornsilk}]"></p>

<p>Lots of moving parts, not so easy to configure and test.
Even with automated provisioning implemented with Vagrant, Docker and Ansible.
If you can&rsquo;t test everything, test at least the most important part of your application &ndash; transformations &ndash; implemented with Spark.</p>

<p>Spark claims, that it is friendly to unit testing with any popular unit test framework.
To be strict, Spark supports rather lightweight integration testing, not unit testing, IMHO.
But still it is much more convenient to test transformation logic locally, than deploying all parts on YARN.</p>

<p>There is a pull request <a href="https://github.com/apache/spark/pull/1751">SPARK-1751</a> that adds &ldquo;unit tests&rdquo; support for Apache Kafka streams.
Should we follow that way? Embedded ZooKeeper and embedded Apache Kafka are needed, the test fixture is complex and cumbersome.
Perhaps tests would be fragile and hard to maintain. This approach makes sense for Spark core team, they want to test Spark and Kafka integration.</p>

<h2>What should be tested?</h2>

<p>Our transformation logic implemented with Spark, nothing more. But how to test the logic so tightly coupled to Spark API (RDD, DStream)?
Let&rsquo;s define how typical Spark application is organized. Our hypothetical application structure looks like this:</p>

<ol>
<li>Initialize <code>SparkContext</code> or <code>StreamingContext</code>.</li>
<li>Create RDD or DStream for given source (e.g: Apache Kafka)</li>
<li>Evaluate transformations on RDD or DStream API.</li>
<li>Put transformation outcomes (e.g: aggregations) into external database.</li>
</ol>


<h3>Context</h3>

<p><code>SparkContext</code> and <code>StreamingContext</code> could be easily initialized for testing purposes.
Set master URL to <code>local</code>, run the operations and then stop context gracefully.</p>

<p>``` scala SparkContext Initialization
class SparkExampleSpec extends FlatSpec with BeforeAndAfter {</p>

<p>  private val master = &ldquo;local[2]&rdquo;
  private val appName = &ldquo;example-spark&rdquo;</p>

<p>  private var sc: SparkContext = _</p>

<p>  before {</p>

<pre><code>val conf = new SparkConf()
  .setMaster(master)
  .setAppName(appName)

sc = new SparkContext(conf)
</code></pre>

<p>  }</p>

<p>  after {</p>

<pre><code>if (sc != null) {
  sc.stop()
}
</code></pre>

<p>  }
  (&hellip;)</p>

<p>```</p>

<p>``` scala StreamingContext Initialization
class SparkStreamingExampleSpec extends FlatSpec with BeforeAndAfter {</p>

<p>  private val master = &ldquo;local[2]&rdquo;
  private val appName = &ldquo;example-spark-streaming&rdquo;
  private val batchDuration = Seconds(1)
  private val checkpointDir = Files.createTempDirectory(appName).toString</p>

<p>  private var sc: SparkContext = _
  private var ssc: StreamingContext = _</p>

<p>  before {</p>

<pre><code>val conf = new SparkConf()
  .setMaster(master)
  .setAppName(appName)

ssc = new StreamingContext(conf, batchDuration)
ssc.checkpoint(checkpointDir)

sc = ssc.sparkContext
</code></pre>

<p>  }</p>

<p>  after {</p>

<pre><code>if (ssc != null) {
  ssc.stop()
}
</code></pre>

<p>  }</p>

<p>  (&hellip;)
```</p>

<h3>RDD and DStream</h3>

<p>The problematic part is how to create RDD or DStream.
For testing purposes it must be simplified to avoid embedded Kafka and ZooKeeper.
Below you can find examples how to create in-memory RDD and DStream.</p>

<p><code>scala In-memory RDD
val lines = Seq("To be or not to be.", "That is the question.")
val rdd = sparkContext.parallelize(lines)
</code></p>

<p>``` scala In-memory DStream
val lines = mutable.Queue<a href="">RDD[String]</a>
val dstream = streamingContext.queueStream(lines)</p>

<p>// append data to DStream
lines += sparkContext.makeRDD(Seq(&ldquo;To be or not to be.&rdquo;, &ldquo;That is the question.&rdquo;))
```</p>

<h3>Transformation logic</h3>

<p>The most important part of our application &ndash; transformations logic &ndash; must be encapsulated in separate class or object.
Object is preferred to avoid class serialization overhead. Exactly the same code is used by the application and by the test.</p>

<p>``` scala WordCount.scala
case class WordCount(word: String, count: Int)</p>

<p>object WordCount {
  def count(lines: RDD[String], stopWords: Set[String]): RDD[WordCount] = {</p>

<pre><code>val words = lines.flatMap(_.split("\\s"))
  .map(_.strip(",").strip(".").toLowerCase)
  .filter(!stopWords.contains(_)).filter(!_.isEmpty)

val wordCounts = words.map(word =&gt; (word, 1)).reduceByKey(_ + _).map {
  case (word: String, count: Int) =&gt; WordCount(word, count)
}

val sortedWordCounts = wordCounts.sortBy(_.word)

sortedWordCounts
</code></pre>

<p>  }
}
```</p>

<h2>Spark test</h2>

<p>Now it is time to implement our first test for WordCount transformation.
The code of test is very straightforward and easy to read.
Single point of truth, the best documentation of your system, always up-to-date.</p>

<p>``` scala
&ldquo;Shakespeare most famous quote&rdquo; should &ldquo;be counted&rdquo; in {</p>

<pre><code>Given("quote")
val lines = Array("To be or not to be.", "That is the question.")

Given("stop words")
val stopWords = Set("the")

When("count words")
val wordCounts = WordCount.count(sc.parallelize(lines), stopWords).collect()

Then("words counted")
wordCounts should equal(Array(
  WordCount("be", 2),
  WordCount("is", 1),
  WordCount("not", 1),
  WordCount("or", 1),
  WordCount("question", 1),
  WordCount("that", 1),
  WordCount("to", 2)))
</code></pre>

<p>  }
```</p>

<h2>Spark Streaming test</h2>

<p>Spark Streaming transformations are much more complex to test.
The full control over clock is needed to manually manage batches, slides and windows.
Without controlled clock you would end up with complex tests with many <code>Thread.sleeep</code> calls.
And the test execution would take ages.
The only downside is that you will not have extra time for coffee during tests execution.</p>

<p>Spark Streaming provides necessary abstraction over system clock, <code>ManualClock</code> class.
Unfortunately <code>ManualClock</code> class is declared as package private. Some hack is needed.
The wrapper presented below, is an adapter for the original <code>ManualClock</code> class but without access restriction.</p>

<p>``` scala ClockWrapper.scala
package org.apache.spark.streaming</p>

<p>import org.apache.spark.streaming.util.ManualClock</p>

<p>class ClockWrapper(ssc: StreamingContext) {</p>

<p>  def getTimeMillis(): Long = manualClock().currentTime()</p>

<p>  def setTime(timeToSet: Long) = manualClock().setTime(timeToSet)</p>

<p>  def advance(timeToAdd: Long) = manualClock().addToTime(timeToAdd)</p>

<p>  def waitTillTime(targetTime: Long): Long = manualClock().waitTillTime(targetTime)</p>

<p>  private def manualClock(): ManualClock = {</p>

<pre><code>ssc.scheduler.clock.asInstanceOf[ManualClock]
</code></pre>

<p>  }</p>

<p>}
```</p>

<p>Now Spark Streaming test can be implemented in efficient way.
The test does not have to wait for system clock and test is implemented with millisecond precision.
You can easily test your windowed scenario from the very beginning to very end.
With given\when\then structure you should be able to understand tested logic without further explanations.</p>

<p>``` scala
&ldquo;Sample set&rdquo; should &ldquo;be counted&rdquo; in {
  Given(&ldquo;streaming context is initialized&rdquo;)
  val lines = mutable.Queue<a href="">RDD[String]</a></p>

<p>  var results = ListBuffer.empty[Array[WordCount]]</p>

<p>  WordCount.count(ssc.queueStream(lines), windowDuration, slideDuration) { (wordsCount: RDD[WordCount], time: Time) =></p>

<pre><code>results += wordsCount.collect()
</code></pre>

<p>  }</p>

<p>  ssc.start()</p>

<p>  When(&ldquo;first set of words queued&rdquo;)
  lines += sc.makeRDD(Seq(&ldquo;a&rdquo;, &ldquo;b&rdquo;))</p>

<p>  Then(&ldquo;words counted after first slide&rdquo;)
  clock.advance(slideDuration.milliseconds)
  eventually(timeout(1 second)) {</p>

<pre><code>results.last should equal(Array(
  WordCount("a", 1),
  WordCount("b", 1)))
</code></pre>

<p>  }</p>

<p>  When(&ldquo;second set of words queued&rdquo;)
  lines += sc.makeRDD(Seq(&ldquo;b&rdquo;, &ldquo;c&rdquo;))</p>

<p>  Then(&ldquo;words counted after second slide&rdquo;)
  clock.advance(slideDuration.milliseconds)
  eventually(timeout(1 second)) {</p>

<pre><code>results.last should equal(Array(
  WordCount("a", 1),
  WordCount("b", 2),
  WordCount("c", 1)))
</code></pre>

<p>  }</p>

<p>  When(&ldquo;nothing more queued&rdquo;)</p>

<p>  Then(&ldquo;word counted after third slide&rdquo;)
  clock.advance(slideDuration.milliseconds)
  eventually(timeout(1 second)) {</p>

<pre><code>results.last should equal(Array(
  WordCount("a", 0),
  WordCount("b", 1),
  WordCount("c", 1)))
</code></pre>

<p>  }</p>

<p>  When(&ldquo;nothing more queued&rdquo;)</p>

<p>  Then(&ldquo;word counted after fourth slide&rdquo;)
  clock.advance(slideDuration.milliseconds)
  eventually(timeout(1 second)) {</p>

<pre><code>results.last should equal(Array(
  WordCount("a", 0),
  WordCount("b", 0),
  WordCount("c", 0)))
</code></pre>

<p>  }
}
```</p>

<p>One comment to <code>Eventually</code> trait usage.
The trait is needed because Spark Streaming is a multithreaded application, and results are not computed immediately.
I found that 1 second timeout is enough for Spark Streaming to calculate the results.
The timeout is not related to batch, slide or window duration.</p>

<h2>Summary</h2>

<p>The complete, working project is published on <a href="https://github.com/mkuthan/example-spark">GitHub</a>.
You can clone/fork the project and do some experiments by yourself.</p>

<p>I hope that Spark committers expose <code>ManualClock</code> for others, eventually.
Control of time is necessary for efficient Spark Streaming application testing.</p>
]]></content>
  </entry>
  
</feed>
