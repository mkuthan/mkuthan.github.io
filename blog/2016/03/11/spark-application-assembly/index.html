
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Spark Application Assembly for Cluster Deployments - Passionate Developer</title>
  <meta name="author" content="Marcin Kuthan">

  
  <meta name="description" content="When I tried to deploy my first Spark application on a YARN cluster,
I realized that there was no clear and concise instruction how to prepare the &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://mkuthan.github.io/blog/2016/03/11/spark-application-assembly">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Passionate Developer" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-50832428-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Passionate Developer</a></h1>
  
    <h2>Memory is unreliable like a software, so make my thoughts more eternal and my software more reliable</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:mkuthan.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Spark Application Assembly for Cluster Deployments</h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-03-11T00:00:00+00:00" pubdate data-updated="true">Mar 11<span>th</span>, 2016</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>When I tried to deploy my first Spark application on a YARN cluster,
I realized that there was no clear and concise instruction how to prepare the application for deployment.
This blog post could be treated as missing manual on how to build Spark application written in Scala to get deployable binary.</p>

<p>This blog post assumes that your Spark application is built with <a href="http://www.scala-sbt.org/">SBT</a>.
As long as SBT is a mainstream tool for building Scala applications the assumption seems legit.
Please ensure that your project is configured with at least SBT 0.13.6.
Open <code>project/build.properties</code> file, verify the version and update SBT if needed:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sbt.version=0.13.11</span></code></pre></td></tr></table></div></figure>


<h2>SBT Assembly Plugin</h2>

<p>The <code>spark-submit</code> script is a convenient way to launch Spark application on the YARN or Mesos cluster.
However, due to distributed nature of the cluster the application has to be prepared as single Java ARchive (JAR).
This archive includes all classes from your project with all of its dependencies.
This application assembly can be prepared using <a href="https://github.com/sbt/sbt-assembly">SBT Assembly Plugin</a>.</p>

<p>To enable SBT Assembly Plugin, add the plugin dependency to the <code>project/plugins.sbt</code> file:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.1")</span></code></pre></td></tr></table></div></figure>


<p>This basic setup can be verified by calling <code>sbt assembly</code> command.
The final assembly location depend on the Scala version, application name and application version.
The build result could be assembled into <code>target/scala-2.11/myapp-assembly-1.0.jar</code> file.</p>

<p>You can configure many aspects of SBT Assembly Plugin like custom merge strategy
but I found that it is much easier to keep the defaults and follow the plugin conventions.
And what is even more important you don&rsquo;t have to change defaults to get correct, deployable application binary assembled by the plugin.</p>

<h2>Provided dependencies scope</h2>

<p>As long as cluster provides Spark classes at runtime, Spark dependencies must be excluded from the assembled JAR.
If not, you should expect weird errors from Java classloader during application startup.
Additional benefit of assembly without Spark dependencies is faster deployment.
Please remember that application assembly must be copied over the network to the location accessible by all cluster nodes (e.g: HDFS or S3).</p>

<p>Look at dependency section in your build file, it should look similar to the code snippet below:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val sparkVersion = "1.6.0"
</span><span class='line'>
</span><span class='line'>"org.apache.spark" %% "spark-core" % sparkVersion,
</span><span class='line'>"org.apache.spark" %% "spark-sql" % sparkVersion,
</span><span class='line'>"org.apache.spark" %% "spark-hive" % sparkVersion,
</span><span class='line'>"org.apache.spark" %% "spark-mlib" % sparkVersion,
</span><span class='line'>"org.apache.spark" %% "spark-graphx" % sparkVersion,
</span><span class='line'>"org.apache.spark" %% "spark-streaming" % sparkVersion,
</span><span class='line'>"org.apache.spark" %% "spark-streaming-kafka" % sparkVersion,
</span><span class='line'>(...)</span></code></pre></td></tr></table></div></figure>


<p>The list of the Spark dependencies is always project specific.
SQL, Hive, MLib, GraphX and Streaming extensions are defined only for reference.
All defined dependencies are required by local build to compile code and run
<a href="http://mkuthan.github.io/blog/2015/03/01/spark-unit-testing/">tests</a>.
So they could not be removed from the build definition in the ordinary way because it will break the build at all.</p>

<p>SBT Assembly Plugin comes with additional dependency scope &ldquo;provided&rdquo;.
The scope is very similar to <a href="https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html">Maven provided scope</a>.
The provided dependency will be part of compilation and test, but excluded from the application assembly.</p>

<p>To configure provided scope for Spark dependencies change the definition as follows:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val sparkVersion = "1.6.0"
</span><span class='line'>
</span><span class='line'>"org.apache.spark" %% "spark-core" % sparkVersion % "provided",
</span><span class='line'>"org.apache.spark" %% "spark-sql" % sparkVersion % "provided",
</span><span class='line'>"org.apache.spark" %% "spark-hive" % sparkVersion % "provided",
</span><span class='line'>"org.apache.spark" %% "spark-mlib" % sparkVersion % "provided",
</span><span class='line'>"org.apache.spark" %% "spark-graphx" % sparkVersion % "provided",
</span><span class='line'>"org.apache.spark" %% "spark-streaming" % sparkVersion % "provided",
</span><span class='line'>"org.apache.spark" %% "spark-streaming-kafka" % sparkVersion
</span><span class='line'>  exclude("log4j", "log4j")
</span><span class='line'>  exclude("org.spark-project.spark", "unused"),
</span><span class='line'>(...)</span></code></pre></td></tr></table></div></figure>


<p>Careful readers should notice that &ldquo;spark-streaming-kafka&rdquo; dependency has not been listed and marked as &ldquo;provided&rdquo;.
It was done by purpose because integration with Kafka is not part of Spark distribution assembly
and has to be assembled into application JAR.
The exclusion rules for &ldquo;spark-streaming-kafka&rdquo; dependency will be discussed later.</p>

<p>Ok, but how to recognize which libraries are part of Spark distribution assembly?
There is no simple answer to this question.
Look for <code>spark-assembly-*-1.6.0.jar</code> file on the cluster classpath,
list the assembly content and verify what is included and what is not.
In the assembly on my cluster I found core, sql, hive, mlib, graphx and streaming classes are embedded but not integration with Kafka.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ tar -tzf spark-assembly-1.6.0.jar
</span><span class='line'>META-INF/
</span><span class='line'>META-INF/MANIFEST.MF
</span><span class='line'>org/
</span><span class='line'>org/apache/
</span><span class='line'>org/apache/spark/
</span><span class='line'>org/apache/spark/HeartbeatReceiver
</span><span class='line'>(...)
</span><span class='line'>org/apache/spark/ml/
</span><span class='line'>org/apache/spark/ml/Pipeline$SharedReadWrite$$anonfun$2.class
</span><span class='line'>org/apache/spark/ml/tuning/
</span><span class='line'>(...)
</span><span class='line'>org/apache/spark/sql/
</span><span class='line'>org/apache/spark/sql/UDFRegistration$$anonfun$3.class
</span><span class='line'>org/apache/spark/sql/SQLContext$$anonfun$range$2.class
</span><span class='line'>(...)
</span><span class='line'>reference.conf
</span><span class='line'>META-INF/NOTICE</span></code></pre></td></tr></table></div></figure>


<h2>SBT run and run-main</h2>

<p>Provided dependency scope unfortunately breaks SBT <code>run</code> and <code>run-main</code> tasks.
Because provided dependencies are excluded from the runtime classpath, you should expect <code>ClassNotFoundException</code> during application startup on local machine.
To fix this issue, provided dependencies must be explicitly added to all SBT tasks used for local run, e.g.:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>run in Compile &lt;&lt;= Defaults.runTask(fullClasspath in Compile, mainClass in(Compile, run), runner in(Compile, run))
</span><span class='line'>runMain in Compile &lt;&lt;= Defaults.runMainTask(fullClasspath in Compile, runner in(Compile, run))</span></code></pre></td></tr></table></div></figure>


<h2>How to exclude Log4j from application assembly?</h2>

<p>Without Spark classes the application assembly is quite lightweight.
But the assembly size might be reduced event more!</p>

<p>Let assume that your application requires some logging provider.
As long as Spark internally uses Log4j, it means that Log4j is already on the cluster classpath.
But you may say that there is much better API for Scala than origin Log4j &ndash; and you are totally right.</p>

<p>The snippet below configure excellent Typesafe (Lightbend nowadays) <a href="https://github.com/typesafehub/scala-logging">Scala Logging Library</a> dependency.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>"com.typesafe.scala-logging" %% "scala-logging" % "3.1.0",
</span><span class='line'>
</span><span class='line'>"org.slf4j" % "slf4j-api" % "1.7.10",
</span><span class='line'>"org.slf4j" % "slf4j-log4j12" % "1.7.10" exclude("log4j", "log4j"),
</span><span class='line'>
</span><span class='line'>"log4j" % "log4j" % "1.2.17" % "provided",</span></code></pre></td></tr></table></div></figure>


<p>Scala Logging is a thin wrapper for SLF4J implemented using Scala macros.
The &ldquo;slf4j-log4j12&rdquo; is a binding library between SLF4J API and Log4j logger provider.
Three layers of indirection but who cares :&ndash;)</p>

<p>There is also top-level dependency to Log4J defined with provided scope.
But this is not enough to get rid of Log4j classes from the application assembly.
Because Log4j is also a transitive dependency of &ldquo;slf4j-log4j12&rdquo; it must be explicitly excluded.
If not, SBT Assembly Plugin adds Log4j classes to the assembly even if top level &ldquo;log4j&rdquo; dependency is marked as &ldquo;provided&rdquo;.
Not very intuitive but SBT Assembly Plugin works this way.</p>

<p>Alternatively you could disable transitive dependencies for &ldquo;slf4j-log4j12&rdquo; at all.
It could be especially useful for libraries with many transitive dependencies which are expected to be on the cluster classpath.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>"org.slf4j" % "slf4j-log4j12" % "1.7.10" intransitive()</span></code></pre></td></tr></table></div></figure>


<h2>Spark Streaming Kafka dependency</h2>

<p>Now we are ready to define dependency to &ldquo;spark-streaming-kafka&rdquo;.
Because Spark integration with Kafka typically is not a part of Spark assembly,
it must be embedded into application assembly.
The artifact should not be defined within &ldquo;provided&rdquo; scope.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>val sparkVersion = "1.6.0"
</span><span class='line'>
</span><span class='line'>(...)
</span><span class='line'>"org.apache.spark" %% "spark-streaming-kafka" % sparkVersion
</span><span class='line'>  exclude("log4j", "log4j")
</span><span class='line'>  exclude("org.spark-project.spark", "unused"),
</span><span class='line'>(...)</span></code></pre></td></tr></table></div></figure>


<p>Again, &ldquo;log4j&rdquo; transitive dependency of Kafka needs to be explicitly excluded.
I also found that marker class from weird Spark &ldquo;unused&rdquo; artifact breaks default SBT Assembly Plugin merge strategy.
It is much easier to exclude this dependency than customize merge strategy of the plugin.</p>

<h2>Where is Guava?</h2>

<p>When you look at your project dependencies you could easily find Guava (version 14.0.1 for Spark 1.6.0).
Ok, Guava is an excellent library so you decide to use the library in your application.</p>

<p><em>WRONG!</em></p>

<p>Guava is on the classpath during compilation and tests but at runtime you will get &ldquo;ClassNotFoundException&rdquo; or method not found error.
First, Guava is shaded in Spark distribution assembly under <code>org/spark-project/guava</code> package and should not be used directly.
Second, there is a huge chance for outdated Guava library on the cluster classpath.
In CDH 5.3 distribution, the installed Guava version is 11.0.2 released on Feb 22, 2012 &ndash; more than 4 years ago!
Since the Guava is <a href="http://i.stack.imgur.com/8K6N8.jpg">binary compatible</a> only between 2 or 3 latest major releases it is a real blocker.</p>

<p>There are experimental configuration flags for Spark <code>spark.driver.userClassPathFirst</code> and <code>spark.executor.userClassPathFirst</code>.
In theory it gives user-added jars precedence over Spark&rsquo;s own jars when loading classes in the the driver.
But in practice it does not work, at least for me :&ndash;(.</p>

<p>In general you should avoid external dependencies at all cost when you develop application deployed on the YARN cluster.
Classloader hell is even bigger than in JEE containers like JBoss or WebLogic.
Look for the libraries with minimal transitive dependencies and narrowed features.
For example, if you need a cache, choose <a href="https://github.com/ben-manes/caffeine">Caffeine</a> over Guava.</p>

<h2>Deployment optimization for YARN cluster</h2>

<p>When application is deployed on YARN cluster using <code>spark-submit</code> script,
the script upload Spark distribution assembly to the cluster during every deployment.
The distribution assembly size is over 100MB, ten times more than typical application assembly!</p>

<p>So I really recommend to install Spark distribution assembly on well known location on the cluster
and define <code>spark.yarn.jar</code> property for <code>spark-submit</code>.
The assembly will not be copied over the network during every deployment.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark.yarn.jar=hdfs:///apps/spark/assembly/spark-assembly-1.6.0.jar</span></code></pre></td></tr></table></div></figure>


<h2>Summary</h2>

<p>I witnessed a few Spark projects where <code>build.sbt</code> were more complex than application itself.
And application assembly was bloated with unnecessary 3rd party classes and deployment process took ages.
Build configuration described in this blog post should help you deploy Spark application on the cluster smoothly
and still keep SBT configuration easy to maintain.</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Marcin Kuthan</span></span>

      








  


<time datetime="2016-03-11T00:00:00+00:00" pubdate data-updated="true">Mar 11<span>th</span>, 2016</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/kafka/'>kafka</a>, <a class='category' href='/blog/categories/sbt/'>sbt</a>, <a class='category' href='/blog/categories/scala/'>scala</a>, <a class='category' href='/blog/categories/spark/'>spark</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://mkuthan.github.io/blog/2016/03/11/spark-application-assembly/" data-via="MarcinKuthan" data-counturl="http://mkuthan.github.io/blog/2016/03/11/spark-application-assembly/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/01/29/spark-kafka-integration2/" title="Previous Post: Spark and Kafka integration patterns, part 2">&laquo; Spark and Kafka integration patterns, part 2</a>
      
      
        <a class="basic-alignment right" href="/blog/2016/09/30/spark-streaming-on-yarn/" title="Next Post: Long-running Spark Streaming jobs on YARN cluster">Long-running Spark Streaming jobs on YARN cluster &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/11/02/kafka-streams-dsl-vs-processor-api/">Kafka Streams DSL vs Processor API</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/11/18/apache-bigdata-europe/">Apache BigData Europe Conference Summary</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/09/30/spark-streaming-on-yarn/">Long-running Spark Streaming Jobs on YARN Cluster</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/03/11/spark-application-assembly/">Spark Application Assembly for Cluster Deployments</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/29/spark-kafka-integration2/">Spark and Kafka Integration Patterns, Part 2</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/08/06/spark-kafka-integration1/">Spark and Kafka Integration Patterns, Part 1</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/01/spark-unit-testing/">Spark and Spark Streaming Unit Testing</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/22/ddd-how-to-learn/">How to Learn DDD</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/15/programming_language_does_not_matter/">Programming Language Does Not Matter</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/08/20/mastering-nodejs-book-review/">Mastering Node.js - Book Review</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/26/soa-patterns-book-review/">SOA Patterns - Book Review</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/21/release-it-book-review/">Rrelease It! - Book Review</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/05/29/acceptance-testing-using-jbehave-spring-framework-and-maven/">Acceptance Testing Using JBehave, Spring Framework and Maven</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/05/27/the-twelve-factor-app-part2/">The Twelve-Factor App - Part 2</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/05/26/the-twelve-factor-app-part1/">The Twelve-Factor App - Part 1</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Category Cloud</h1>
    <span id="tag-cloud"><a href='/blog/categories/architecture' style='font-size: 148.0%'>architecture(4)</a> <a href='/blog/categories/artifactory' style='font-size: 112.0%'>artifactory(1)</a> <a href='/blog/categories/bash' style='font-size: 112.0%'>bash(1)</a> <a href='/blog/categories/books' style='font-size: 136.0%'>books(3)</a> <a href='/blog/categories/conferences' style='font-size: 112.0%'>conferences(1)</a> <a href='/blog/categories/craftsmanship' style='font-size: 136.0%'>craftsmanship(3)</a> <a href='/blog/categories/ddd' style='font-size: 124.0%'>DDD(2)</a> <a href='/blog/categories/dvcs' style='font-size: 112.0%'>DVCS(1)</a> <a href='/blog/categories/git' style='font-size: 124.0%'>git(2)</a> <a href='/blog/categories/hdfs' style='font-size: 112.0%'>hdfs(1)</a> <a href='/blog/categories/java' style='font-size: 124.0%'>java(2)</a> <a href='/blog/categories/jbehave' style='font-size: 112.0%'>jbehave(1)</a> <a href='/blog/categories/jee' style='font-size: 112.0%'>JEE(1)</a> <a href='/blog/categories/jira' style='font-size: 112.0%'>jira(1)</a> <a href='/blog/categories/jms' style='font-size: 112.0%'>JMS(1)</a> <a href='/blog/categories/jvm' style='font-size: 112.0%'>jvm(1)</a> <a href='/blog/categories/kafka' style='font-size: 148.0%'>kafka(4)</a> <a href='/blog/categories/kafka-streams' style='font-size: 112.0%'>kafka streams(1)</a> <a href='/blog/categories/linux' style='font-size: 136.0%'>linux(3)</a> <a href='/blog/categories/maven' style='font-size: 124.0%'>maven(2)</a> <a href='/blog/categories/node-js' style='font-size: 112.0%'>node.js(1)</a> <a href='/blog/categories/performance' style='font-size: 112.0%'>performance(1)</a> <a href='/blog/categories/php' style='font-size: 112.0%'>PHP(1)</a> <a href='/blog/categories/presentations' style='font-size: 112.0%'>presentations(1)</a> <a href='/blog/categories/python' style='font-size: 112.0%'>python(1)</a> <a href='/blog/categories/ruby' style='font-size: 112.0%'>ruby(1)</a> <a href='/blog/categories/sbt' style='font-size: 112.0%'>sbt(1)</a> <a href='/blog/categories/scala' style='font-size: 160.0%'>scala(5)</a> <a href='/blog/categories/smtp' style='font-size: 112.0%'>smtp(1)</a> <a href='/blog/categories/spark' style='font-size: 160.0%'>spark(5)</a> <a href='/blog/categories/spring' style='font-size: 148.0%'>spring(4)</a> <a href='/blog/categories/tdd' style='font-size: 112.0%'>TDD(1)</a> <a href='/blog/categories/virtual-box' style='font-size: 112.0%'>virtual box(1)</a> <a href='/blog/categories/yarn' style='font-size: 112.0%'>yarn(1)</a> </span>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/mkuthan">@mkuthan</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'mkuthan',
            count: 10,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>About Me</h1>
  <p>Marcin Kuthan, genuine software engineer at <a href="http://allegrogroup.com/">Allegro Group</a>.</p>
</section>

<section>
	<span>
		<img src="http://www.gravatar.com/avatar/71e9ba5350f53c3416b7ed2617d04ab5" alt="Gravatar of Marcin Kuthan " title="Gravatar of Marcin Kuthan" />
	</span>
</section>
<section>
  <h1>Links</h1>
  <p><a href="http://allegro.tech/"><img src="http://allegro.tech/img/logo-allegro-tech.svg" alt="allegro.tech"></a></p>
</section>

  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Marcin Kuthan -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'mkuthan';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://mkuthan.github.io/blog/2016/03/11/spark-application-assembly/';
        var disqus_url = 'http://mkuthan.github.io/blog/2016/03/11/spark-application-assembly/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
