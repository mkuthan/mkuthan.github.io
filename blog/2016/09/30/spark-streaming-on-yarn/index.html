
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Long-running Spark Streaming Jobs on YARN Cluster - Passionate Developer</title>
  <meta name="author" content="Marcin Kuthan">

  
  <meta name="description" content="A long-running Spark Streaming job, once submitted to the YARN cluster should run forever until it is intentionally stopped.
Any interruption &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Passionate Developer" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-50832428-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Passionate Developer</a></h1>
  
    <h2>Memory is unreliable like a software, so make my thoughts more eternal and my software more reliable</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:mkuthan.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Long-running Spark Streaming Jobs on YARN Cluster</h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-09-30T00:00:00+00:00" pubdate data-updated="true">Sep 30<span>th</span>, 2016</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>A long-running Spark Streaming job, once submitted to the YARN cluster should run forever until it is intentionally stopped.
Any interruption introduces substantial processing delays and could lead to data loss or duplicates.
Neither YARN nor Apache Spark have been designed for executing long-running services.
But they have been successfully adapted to growing needs of near real-time processing implemented as long-running jobs.
Successfully does not necessarily mean without technological challenges.</p>

<p>This blog post summarizes my experiences in running mission critical, long-running Spark Streaming jobs on a secured YARN cluster.
You will learn how to submit Spark Streaming application to a YARN cluster to avoid sleepless nights during on-call hours.</p>

<h2>Fault tolerance</h2>

<p>In the YARN cluster mode Spark driver runs in the same container as the Application Master,
the first YARN container allocated by the application.
This process is responsible for driving the application and requesting resources (Spark executors) from YARN.
What is important, Application Master eliminates need for any another process that run during application lifecycle.
Even if an edge Hadoop cluster node where the Spark Streaming job was submitted fails, the application stays unaffected.</p>

<p>To run Spark Streaming application in the cluster mode, ensure that the following parameters are given to spark-submit command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark-submit --master yarn --deploy-mode cluster</span></code></pre></td></tr></table></div></figure>


<p>Because Spark driver and Application Master share a single JVM, any error in Spark driver stops our long-running job.
Fortunately it is possible to configure maximum number of attempts that will be made to re-run the application.
It is reasonable to set higher value than default 2 (derived from YARN cluster property <code>yarn.resourcemanager.am.max-attempts</code>).
For me 4 works quite well, higher value may cause unnecessary restarts even if the reason of the failure is permanent.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark-submit --master yarn --deploy-mode cluster \
</span><span class='line'>    --conf spark.yarn.maxAppAttempts=4</span></code></pre></td></tr></table></div></figure>


<p>If the application runs for days or weeks without restart or redeployment on highly utilized cluster, 4 attempts could be exhausted in few hours.
To avoid this situation, the attempt counter should be reset on every hour of so.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark-submit --master yarn --deploy-mode cluster \
</span><span class='line'>    --conf spark.yarn.maxAppAttempts=4 \
</span><span class='line'>    --conf spark.yarn.am.attemptFailuresValidityInterval=1h</span></code></pre></td></tr></table></div></figure>


<p>Another important setting is a maximum number of executor failures before the application fails.
By default it is <code>max(2 * num executors, 3)</code>, well suited for batch jobs but not for long-running jobs.
The property comes with corresponding validity interval which also should be set.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark-submit --master yarn --deploy-mode cluster \
</span><span class='line'>    --conf spark.yarn.maxAppAttempts=4 \
</span><span class='line'>    --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
</span><span class='line'>    --conf spark.yarn.max.executor.failures={8 * num_executors} \
</span><span class='line'>    --conf spark.yarn.executor.failuresValidityInterval=1h</span></code></pre></td></tr></table></div></figure>


<p>For long-running jobs you could also consider to boost maximum number of task failures before giving up the job.
By default tasks will be retried 4 times and then job fails.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark-submit --master yarn --deploy-mode cluster \
</span><span class='line'>    --conf spark.yarn.maxAppAttempts=4 \
</span><span class='line'>    --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
</span><span class='line'>    --conf spark.yarn.max.executor.failures={8 * num_executors} \
</span><span class='line'>    --conf spark.yarn.executor.failuresValidityInterval=1h \
</span><span class='line'>    --conf spark.task.maxFailures=8</span></code></pre></td></tr></table></div></figure>


<p></p>

<h2>Performance</h2>

<p>When a Spark Streaming application is submitted to the cluster, YARN queue where the job runs must be defined.
I strongly recommend using YARN <a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
and submitting long-running jobs to separate queue.
Without a separate YARN queue your long-running job will be preempted by a massive Hive query sooner or later.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark-submit --master yarn --deploy-mode cluster \
</span><span class='line'>    --conf spark.yarn.maxAppAttempts=4 \
</span><span class='line'>    --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
</span><span class='line'>    --conf spark.yarn.max.executor.failures={8 * num_executors} \
</span><span class='line'>    --conf spark.yarn.executor.failuresValidityInterval=1h \
</span><span class='line'>    --conf spark.task.maxFailures=8 \
</span><span class='line'>    --queue realtime_queue</span></code></pre></td></tr></table></div></figure>


<p>Another important issue for Spark Streaming job is keeping processing time stable and highly predictable.
Processing time should stay below batch duration to avoid delays.
I&rsquo;ve found that Spark speculative execution helps a lot, especially on a busy cluster.
Batch processing times are much more stable when speculative execution is enabled.
Unfortunately speculative mode can be enabled only if Spark actions are idempotent.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark-submit --master yarn --deploy-mode cluster \
</span><span class='line'>    --conf spark.yarn.maxAppAttempts=4 \
</span><span class='line'>    --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
</span><span class='line'>    --conf spark.yarn.max.executor.failures={8 * num_executors} \
</span><span class='line'>    --conf spark.yarn.executor.failuresValidityInterval=1h \
</span><span class='line'>    --conf spark.task.maxFailures=8 \
</span><span class='line'>    --queue realtime_queue \
</span><span class='line'>    --conf spark.speculation=true</span></code></pre></td></tr></table></div></figure>


<h2>Security</h2>

<p>On a secured HDFS cluster, long-running Spark Streaming jobs fails due to Kerberos ticket expiration.
Without additional settings, Kerberos ticket is issued when Spark Streaming job is submitted to the cluster.
When ticket expires Spark Streaming job is not able to write or read data from HDFS anymore.</p>

<p>In theory (based on documentation) it should be enough to pass Kerberos principal and keytab as spark-submit command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark-submit --master yarn --deploy-mode cluster \
</span><span class='line'>     --conf spark.yarn.maxAppAttempts=4 \
</span><span class='line'>     --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
</span><span class='line'>     --conf spark.yarn.max.executor.failures={8 * num_executors} \
</span><span class='line'>     --conf spark.yarn.executor.failuresValidityInterval=1h \
</span><span class='line'>     --conf spark.task.maxFailures=8 \
</span><span class='line'>     --queue realtime_queue \
</span><span class='line'>     --conf spark.speculation=true \
</span><span class='line'>     --principal user/hostname@domain \
</span><span class='line'>     --keytab /path/to/foo.keytab</span></code></pre></td></tr></table></div></figure>


<p>In practice, due to several bugs (<a href="https://issues.apache.org/jira/browse/HDFS-9276">HDFS-9276</a>, <a href="https://issues.apache.org/jira/browse/SPARK-11182">SPARK-11182</a>)
HDFS cache must be disabled. If not, Spark will not be able to read updated token from file on HDFS.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark-submit --master yarn --deploy-mode cluster \
</span><span class='line'>     --conf spark.yarn.maxAppAttempts=4 \
</span><span class='line'>     --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
</span><span class='line'>     --conf spark.yarn.max.executor.failures={8 * num_executors} \
</span><span class='line'>     --conf spark.yarn.executor.failuresValidityInterval=1h \
</span><span class='line'>     --conf spark.task.maxFailures=8 \
</span><span class='line'>     --queue realtime_queue \
</span><span class='line'>     --conf spark.speculation=true \
</span><span class='line'>     --principal user/hostname@domain \
</span><span class='line'>     --keytab /path/to/foo.keytab \
</span><span class='line'>     --conf spark.hadoop.fs.hdfs.impl.disable.cache=true</span></code></pre></td></tr></table></div></figure>


<p>Mark Grover pointed out that those bugs only affect HDFS cluster configured with NameNodes in
<a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">HA mode</a>.
Thanks, Mark.</p>

<h2>Logging</h2>

<p>The easiest way to access Spark application logs is to configure Log4j console appender,
wait for application termination and use <code>yarn logs -applicationId [applicationId]</code> command.
Unfortunately it is not feasible to terminate long-running Spark Streaming jobs to access the logs.</p>

<p>I recommend to install and configure Elastic, Logstash and Kibana (<a href="https://www.elastic.co/">ELK</a> stack).
ELK installation and configuration is out of this blog post scope,
but remember to log the following context fields:</p>

<ul>
<li>YARN application id</li>
<li>YARN container hostname</li>
<li>Executor id (Spark driver is always 000001, Spark executors start from 000002)</li>
<li>YARN attempt (to check how many times Spark driver has been restarted)</li>
</ul>


<p>Log4j configuration with Logstash specific appender and layout definition should be passed to spark-submit command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark-submit --master yarn --deploy-mode cluster \
</span><span class='line'>     --conf spark.yarn.maxAppAttempts=4 \
</span><span class='line'>     --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
</span><span class='line'>     --conf spark.yarn.max.executor.failures={8 * num_executors} \
</span><span class='line'>     --conf spark.yarn.executor.failuresValidityInterval=1h \
</span><span class='line'>     --conf spark.task.maxFailures=8 \
</span><span class='line'>     --queue realtime_queue \
</span><span class='line'>     --conf spark.speculation=true \
</span><span class='line'>     --principal user/hostname@domain \
</span><span class='line'>     --keytab /path/to/foo.keytab \
</span><span class='line'>     --conf spark.hadoop.fs.hdfs.impl.disable.cache=true \
</span><span class='line'>     --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties \
</span><span class='line'>     --conf spark.executor.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties \
</span><span class='line'>     --files /path/to/log4j.properties</span></code></pre></td></tr></table></div></figure>


<p>Finally Kibana dashboard for Spark Job might look like:</p>

<p><img src="/images/blog/spark_job_logging.png" title="[spark job logging]" ></p>

<h2>Monitoring</h2>

<p>Long running job runs 24/7 so it is important to have an insight into historical metrics.
Spark UI keeps statistics only for limited number of batches, and after restart all metrics are gone.
Again, external tools are needed.
I recommend to install <a href="https://graphiteapp.org/">Graphite</a> for collecting metrics
and <a href="http://grafana.org/">Grafana</a> for building dashboards.</p>

<p>First, Spark needs to be configured to report metrics into Graphite, prepare the <code>metrics.properties</code> file:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>*.sink.graphite.class=org.apache.spark.metrics.sink.GraphiteSink
</span><span class='line'>*.sink.graphite.host=[hostname]
</span><span class='line'>*.sink.graphite.port=[port]
</span><span class='line'>*.sink.graphite.prefix=some_meaningful_name
</span><span class='line'>
</span><span class='line'>driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource
</span><span class='line'>executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource</span></code></pre></td></tr></table></div></figure>


<p>And configure spark-submit command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark-submit --master yarn --deploy-mode cluster \
</span><span class='line'>     --conf spark.yarn.maxAppAttempts=4 \
</span><span class='line'>     --conf spark.yarn.am.attemptFailuresValidityInterval=1h \
</span><span class='line'>     --conf spark.yarn.max.executor.failures={8 * num_executors} \
</span><span class='line'>     --conf spark.yarn.executor.failuresValidityInterval=1h \
</span><span class='line'>     --conf spark.task.maxFailures=8 \
</span><span class='line'>     --queue realtime_queue \
</span><span class='line'>     --conf spark.speculation=true \
</span><span class='line'>     --principal user/hostname@domain \
</span><span class='line'>     --keytab /path/to/foo.keytab \
</span><span class='line'>     --conf spark.hadoop.fs.hdfs.impl.disable.cache=true \
</span><span class='line'>     --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties \
</span><span class='line'>     --conf spark.executor.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties \
</span><span class='line'>     --files /path/to/log4j.properties:/path/to/metrics.properties</span></code></pre></td></tr></table></div></figure>


<h3>Metrics</h3>

<p>Spark publishes tons of metrics from driver and executors.
If I were to choose the most important one, it would be the last received batch records.
When <code>StreamingMetrics.streaming.lastReceivedBatch_records == 0</code> it probably means that Spark Streaming job has been stopped or failed.</p>

<p>Other important metrics are listed below:</p>

<ul>
<li>When total delay is greater than batch interval, latency of the processing pipeline increases.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>driver.StreamingMetrics.streaming.lastCompletedBatch_totalDelay</span></code></pre></td></tr></table></div></figure>


<ul>
<li>When number of active tasks is lower than <code>number of executors * number of cores</code>, allocated YARN resources are not fully utilized.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>executor.threadpool.activeTasks</span></code></pre></td></tr></table></div></figure>


<ul>
<li>How much RAM is used for RDD cache.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>driver.BlockManager.memory.memUsed_MB</span></code></pre></td></tr></table></div></figure>


<ul>
<li>When there is not enough RAM for RDD cache, how much data has been spilled to disk.
You should increase executor memory or change <code>spark.memory.fraction</code> Spark property to avoid performance degradation.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>driver.BlockManager.disk.diskSpaceUsed_MB</span></code></pre></td></tr></table></div></figure>


<ul>
<li>What is JVM memory utilization on Spark driver.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>driver.jvm.heap.used
</span><span class='line'>driver.jvm.non-heap.used
</span><span class='line'>driver.jvm.pools.G1-Old-Gen.used
</span><span class='line'>driver.jvm.pools.G1-Eden-Space.used
</span><span class='line'>driver.jvm.pools.G1-Survivor-Space.used</span></code></pre></td></tr></table></div></figure>


<ul>
<li>How much time is spent on GC on Spark driver.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>driver.jvm.G1-Old-Generation.time
</span><span class='line'>driver.jvm.G1-Young-Generation.time</span></code></pre></td></tr></table></div></figure>


<ul>
<li>What is JMV memory utilization on Spark executors.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[0-9]*.jvm.heap.used
</span><span class='line'>[0-9]*.jvm.non-heap.used
</span><span class='line'>[0-9]*.jvm.pools.G1-Old-Gen.used
</span><span class='line'>[0-9]*.jvm.pools.G1-Survivor-Space.used
</span><span class='line'>[0-9]*.jvm.pools.G1-Eden-Space.used</span></code></pre></td></tr></table></div></figure>


<ul>
<li>How much time is spent on GC on Spark executors.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[0-9]*.jvm.G1-Old-Generation.time
</span><span class='line'>[0-9]*.jvm.G1-Young-Generation.time</span></code></pre></td></tr></table></div></figure>


<h3>Grafana</h3>

<p>While you configure first Grafana dashboard for Spark application, the first problem pops up:</p>

<blockquote><p>How to configure Graphite query when metrics for every Spark application run are reported under its own application id?</p></blockquote>

<p>If you are lucky and brave enough to use Spark 2.1, pin the application metric into static application name:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>--conf spark.metrics.namespace=my_application_name</span></code></pre></td></tr></table></div></figure>


<p>For Spark versions older than 2.1, a few tricks with Graphite built-in functions are needed.</p>

<p>Driver metrics use wildcard <code>.*(application_[0-9]+).*</code>
and <code>aliasSub</code> Graphite function to present &lsquo;application id&rsquo; as graph legend:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>aliasSub(stats.analytics.$job_name.*.prod.$dc.*.driver.jvm.heap.used, ".*(application_[0-9]+).*", "heap: \1")</span></code></pre></td></tr></table></div></figure>


<p>For executor metrics again use wildcard <code>.*(application_[0-9]+).*</code>,
<code>groupByNode</code> Graphite function to sum metrics from all Spark executors
and finally <code>aliasSub</code> Graphite function to present &lsquo;application id&rsquo; as graph legend:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>aliasSub(groupByNode(stats.analytics.$job_name.*.prod.$dc.*.[0-9]*.jvm.heap.used, 6, "sumSeries"), "(.*)", "heap: \1")</span></code></pre></td></tr></table></div></figure>


<p>Finally Grafana dashboard for Spark Job might look like:</p>

<p><img src="/images/blog/spark_job_monitoring.png" title="[spark job monitoring]" ></p>

<p>If Spark application is restarted frequently, metrics for old, already finished runs should be deleted from Graphite.
Because Graphite does not compact inactive metrics, old metrics slow down Graphite itself and Grafana queries.</p>

<h2>Graceful stop</h2>

<p>The last puzzle element is how to stop Spark Streaming application deployed on YARN in a graceful way.
The standard method for stopping (or rather killing) YARN application is using a command <code>yarn application -kill [applicationId]</code>.
And this command stops the Spark Streaming application but this could happen in the middle of a batch.
So if the job reads data from Kafka, saves processing results on HDFS and finally commits Kafka offsets
you should expect duplicated data on HDFS when job was stopped just before committing offsets.</p>

<p>The first attempt to solve graceful shutdown issue was to call Spark streaming context stop method in a shutdown hook.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">sys</span><span class="o">.</span><span class="n">addShutdownHook</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">streamingContext</span><span class="o">.</span><span class="n">stop</span><span class="o">(</span><span class="n">stopSparkContext</span> <span class="k">=</span> <span class="kc">true</span><span class="o">,</span> <span class="n">stopGracefully</span> <span class="k">=</span> <span class="kc">true</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Disappointingly a shutdown hook is called too late to finish started batch and Spark application is killed almost immediately.
Moreover there is no guarantee that a shutdown hook will be called by JVM at all.</p>

<p>At the time of writing this blog post the only confirmed way to shutdown gracefully Spark Streaming application on YARN
is to notifying somehow the application about planned shutdown, and then stop streaming context programmatically (but not from shutdown hook).
Command <code>yarn application -kill</code> should be used only as a last resort if notified application did not stop after defined timeout.</p>

<p>The application can be notified about planned shutdown using marker file on HDFS (the easiest way),
or using simple Socket/HTTP endpoint exposed on the driver (sophisticated way).</p>

<p>Because I like KISS principle, below you can find shell script pseudo-code for starting / stopping Spark Streaming application using marker file:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>start<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    hdfs dfs -touchz /path/to/marker/my_job_unique_name
</span><span class='line'>    spark-submit ...
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'>stop<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    hdfs dfs -rm /path/to/marker/my_job_unique_name
</span><span class='line'>    <span class="nv">force_kill</span><span class="o">=</span><span class="nb">true</span>
</span><span class='line'><span class="nb">    </span><span class="nv">application_id</span><span class="o">=</span><span class="k">$(</span>yarn application -list | grep -oe <span class="s2">&quot;application_[0-9]*_[0-9]*&quot;</span><span class="sb">`</span><span class="o">)</span>
</span><span class='line'>    <span class="k">for </span>i in <span class="sb">`</span>seq 1 10<span class="sb">`</span>; <span class="k">do</span>
</span><span class='line'><span class="k">        </span><span class="nv">application_status</span><span class="o">=</span><span class="k">$(</span>yarn application -status <span class="k">${</span><span class="nv">application_id</span><span class="k">}</span> | grep <span class="s2">&quot;State : \(RUNNING\|ACCEPTED\)&quot;</span><span class="k">)</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">[</span> -n <span class="s2">&quot;$application_status&quot;</span> <span class="o">]</span>; <span class="k">then</span>
</span><span class='line'><span class="k">            </span>sleep 60s
</span><span class='line'>        <span class="k">else</span>
</span><span class='line'><span class="k">            </span><span class="nv">force_kill</span><span class="o">=</span><span class="nb">false</span>
</span><span class='line'><span class="nb">            break</span>
</span><span class='line'><span class="nb">        </span><span class="k">fi</span>
</span><span class='line'><span class="k">    done</span>
</span><span class='line'>    <span class="nv">$force_kill</span> <span class="o">&amp;&amp;</span> yarn application -kill <span class="k">${</span><span class="nv">application_id</span><span class="k">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>In the Spark Streaming application, background thread should monitor marker file,
and when the file disappears stop the context calling <code>streamingContext.stop(stopSparkContext = true, stopGracefully = true)</code>.</p>

<h2>Summary</h2>

<p>As you could see, configuration for mission critical Spark Streaming application deployed on YARN is quite complex.
It has been long, tedious and iterative learning process of all presented techniques by a few very smart devs.
But at the end, long-running Spark Streaming applications deployed on highly utilized YARN cluster are extraordinarily stable.</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Marcin Kuthan</span></span>

      








  


<time datetime="2016-09-30T00:00:00+00:00" pubdate data-updated="true">Sep 30<span>th</span>, 2016</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/hdfs/'>hdfs</a>, <a class='category' href='/blog/categories/spark/'>spark</a>, <a class='category' href='/blog/categories/yarn/'>yarn</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/" data-via="MarcinKuthan" data-counturl="http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/03/11/spark-application-assembly/" title="Previous Post: Spark application assembly for cluster deployments">&laquo; Spark application assembly for cluster deployments</a>
      
      
        <a class="basic-alignment right" href="/blog/2016/11/18/apache-bigdata-europe/" title="Next Post: Apache BigData Europe Conference Summary">Apache BigData Europe Conference Summary &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/11/02/kafka-streams-dsl-vs-processor-api/">Kafka Streams DSL vs Processor API</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/11/18/apache-bigdata-europe/">Apache BigData Europe Conference Summary</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/09/30/spark-streaming-on-yarn/">Long-running Spark Streaming Jobs on YARN Cluster</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/03/11/spark-application-assembly/">Spark Application Assembly for Cluster Deployments</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/29/spark-kafka-integration2/">Spark and Kafka Integration Patterns, Part 2</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/08/06/spark-kafka-integration1/">Spark and Kafka Integration Patterns, Part 1</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/01/spark-unit-testing/">Spark and Spark Streaming Unit Testing</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/22/ddd-how-to-learn/">How to Learn DDD</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/15/programming_language_does_not_matter/">Programming Language Does Not Matter</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/08/20/mastering-nodejs-book-review/">Mastering Node.js - Book Review</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/26/soa-patterns-book-review/">SOA Patterns - Book Review</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/06/21/release-it-book-review/">Rrelease It! - Book Review</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/05/29/acceptance-testing-using-jbehave-spring-framework-and-maven/">Acceptance Testing Using JBehave, Spring Framework and Maven</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/05/27/the-twelve-factor-app-part2/">The Twelve-Factor App - Part 2</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/05/26/the-twelve-factor-app-part1/">The Twelve-Factor App - Part 1</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Category Cloud</h1>
    <span id="tag-cloud"><a href='/blog/categories/architecture' style='font-size: 148.0%'>architecture(4)</a> <a href='/blog/categories/artifactory' style='font-size: 112.0%'>artifactory(1)</a> <a href='/blog/categories/bash' style='font-size: 112.0%'>bash(1)</a> <a href='/blog/categories/books' style='font-size: 136.0%'>books(3)</a> <a href='/blog/categories/conferences' style='font-size: 112.0%'>conferences(1)</a> <a href='/blog/categories/craftsmanship' style='font-size: 136.0%'>craftsmanship(3)</a> <a href='/blog/categories/ddd' style='font-size: 124.0%'>DDD(2)</a> <a href='/blog/categories/dvcs' style='font-size: 112.0%'>DVCS(1)</a> <a href='/blog/categories/git' style='font-size: 124.0%'>git(2)</a> <a href='/blog/categories/hdfs' style='font-size: 112.0%'>hdfs(1)</a> <a href='/blog/categories/java' style='font-size: 124.0%'>java(2)</a> <a href='/blog/categories/jbehave' style='font-size: 112.0%'>jbehave(1)</a> <a href='/blog/categories/jee' style='font-size: 112.0%'>JEE(1)</a> <a href='/blog/categories/jira' style='font-size: 112.0%'>jira(1)</a> <a href='/blog/categories/jms' style='font-size: 112.0%'>JMS(1)</a> <a href='/blog/categories/jvm' style='font-size: 112.0%'>jvm(1)</a> <a href='/blog/categories/kafka' style='font-size: 148.0%'>kafka(4)</a> <a href='/blog/categories/kafka-streams' style='font-size: 112.0%'>kafka streams(1)</a> <a href='/blog/categories/linux' style='font-size: 136.0%'>linux(3)</a> <a href='/blog/categories/maven' style='font-size: 124.0%'>maven(2)</a> <a href='/blog/categories/node-js' style='font-size: 112.0%'>node.js(1)</a> <a href='/blog/categories/performance' style='font-size: 112.0%'>performance(1)</a> <a href='/blog/categories/php' style='font-size: 112.0%'>PHP(1)</a> <a href='/blog/categories/presentations' style='font-size: 112.0%'>presentations(1)</a> <a href='/blog/categories/python' style='font-size: 112.0%'>python(1)</a> <a href='/blog/categories/ruby' style='font-size: 112.0%'>ruby(1)</a> <a href='/blog/categories/sbt' style='font-size: 112.0%'>sbt(1)</a> <a href='/blog/categories/scala' style='font-size: 160.0%'>scala(5)</a> <a href='/blog/categories/smtp' style='font-size: 112.0%'>smtp(1)</a> <a href='/blog/categories/spark' style='font-size: 160.0%'>spark(5)</a> <a href='/blog/categories/spring' style='font-size: 148.0%'>spring(4)</a> <a href='/blog/categories/tdd' style='font-size: 112.0%'>TDD(1)</a> <a href='/blog/categories/virtual-box' style='font-size: 112.0%'>virtual box(1)</a> <a href='/blog/categories/yarn' style='font-size: 112.0%'>yarn(1)</a> </span>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/mkuthan">@mkuthan</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'mkuthan',
            count: 10,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <h1>About Me</h1>
  <p>Marcin Kuthan, genuine software engineer at <a href="http://allegrogroup.com/">Allegro Group</a>.</p>
</section>

<section>
	<span>
		<img src="http://www.gravatar.com/avatar/71e9ba5350f53c3416b7ed2617d04ab5" alt="Gravatar of Marcin Kuthan " title="Gravatar of Marcin Kuthan" />
	</span>
</section>
<section>
  <h1>Links</h1>
  <p><a href="http://allegro.tech/"><img src="http://allegro.tech/img/logo-allegro-tech.svg" alt="allegro.tech"></a></p>
</section>

  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Marcin Kuthan -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'mkuthan';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/';
        var disqus_url = 'http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
